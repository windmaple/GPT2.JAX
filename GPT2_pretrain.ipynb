{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrain the GPT2 model\n",
        "\n",
        "This notebook seeks to replicate Andrey Karpathy's highly popular [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) project and trains a GPT2 model from scratch using JAX+TPU and the [OpenWebText dataset](https://huggingface.co/datasets/Skylion007/openwebtext).\n",
        "\n",
        "You can run this on Colab, Kaggle or GCP TPUs, although Colab TPU v2 can only run the smallest 124M GPT model.\n"
      ],
      "metadata": {
        "id": "rvP1eNN_pExM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine platform"
      ],
      "metadata": {
        "id": "LD3bo9FxhrTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists('/content/'):\n",
        "  platform = \"Colab\"\n",
        "elif os.path.exists('/kaggle/'):\n",
        "  platform = \"Kaggle\"\n",
        "else:\n",
        "  # Assume using Cloud TPU otherwise\n",
        "  platform = \"GCP\""
      ],
      "metadata": {
        "id": "7XcEXnSbhhKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ],
      "metadata": {
        "id": "hTmz5Cbco7n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jax-ai-stack\n",
        "if platform == \"Colab\": # temp workaround on Colab (https://github.com/jax-ml/jax-ai-stack/issues/149)\n",
        "  !pip install -Uq \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info\n"
      ],
      "metadata": {
        "id": "6zMsOIc7ouCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm we have TPUs set up."
      ],
      "metadata": {
        "id": "6cWxBvz6bZDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "uZUaKdi5bSEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take care of the imports."
      ],
      "metadata": {
        "id": "sKE2uUafLobI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax, orbax\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import numpy as np\n",
        "import tiktoken, time, wandb"
      ],
      "metadata": {
        "id": "MKYFNOhdLq98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Define the device mesh.\n"
      ],
      "metadata": {
        "id": "rPyt7MV6prz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Alternative data and model parallel\n",
        "# mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
      ],
      "metadata": {
        "id": "xuMlCK3Q8WJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
      ],
      "metadata": {
        "id": "_ZKdhNo98NgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "iWbkk1V7-Isg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set some hyperparameters."
      ],
      "metadata": {
        "id": "igX_eoGNMTGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "GPT2_variant = \"GPT2\" # \"GPT2-medium\"\n",
        "if GPT2_variant == \"GPT2-medium\":\n",
        "  num_transformer_blocks = 24\n",
        "  seqlen = 1024\n",
        "  embed_dim = 1024\n",
        "  num_heads = 16\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  batch_size = 32  # Can only run on TPU v3+\n",
        "else: ## Assume GPT2 otherwise\n",
        "  num_transformer_blocks = 12\n",
        "  seqlen = 1024\n",
        "  embed_dim = 768\n",
        "  num_heads = 12\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  if platform == \"Colab\":\n",
        "      batch_size = 24 # TPU v2\n",
        "  else:\n",
        "      batch_size = 72 # TPU v3\n",
        "\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_steps = 600000*12//batch_size\n",
        "# Kaggle TPU limit per session is 9 hours, which is ~95K steps for GPT2\n",
        "if platform == \"Kaggle\":\n",
        "  max_steps = 90000\n",
        "init_learning_rate = 5e-4\n",
        "weight_decay = 1e-1\n",
        "top_k = 10\n",
        "dtype = jnp.bfloat16"
      ],
      "metadata": {
        "id": "GRhiDsCrMZRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define the model architecture. You can refer to [OpenAI's official implementation](https://github.com/openai/gpt-2) or [nanoGPT](https://github.com/karpathy/nanoGPT) for comparison. Main difference is with the sharding scheme."
      ],
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float, rngs: nnx.Rngs):\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                          dtype=dtype,\n",
        "                                          param_dtype=dtype,\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            decode=False,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "\n",
        "        return x + mlp_output\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, dtype=dtype, param_dtype=dtype, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=seqlen, features=embed_dim, dtype=dtype, param_dtype=dtype, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding+position_embedding\n",
        "\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, num_heads: int, rate: float, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    seqlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.dropout = nnx.Dropout(rate=rate)\n",
        "\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.layer_norm = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                    num_features=embed_dim,\n",
        "                                    scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    dtype=dtype,\n",
        "                                    param_dtype=dtype,\n",
        "                                    rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        # Weights tying\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, padded_tokens, sample_index):\n",
        "        logits = self(padded_tokens)\n",
        "        next_token = self.sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        generated = []\n",
        "        for i in range(max_tokens):\n",
        "            sample_index = len(start_tokens) + len(generated) - 1\n",
        "            # TODO: use attention masking for better efficiency\n",
        "            padded_tokens = jnp.array((start_tokens + generated + [0] * (seqlen - len(start_tokens) - len(generated))))[None, :]\n",
        "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(next_token)\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return GPT2(seqlen, vocab_size, embed_dim, num_heads, dropout_rate, feed_forward_dim, num_transformer_blocks, rngs=rngs)"
      ],
      "metadata": {
        "id": "z0p-IHurrB9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Weights and Biases to track training progress."
      ],
      "metadata": {
        "id": "eBfT1dp5hMUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import userdata\n",
        "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "  os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "  os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "elif platform == \"Kaggle\":\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  os.environ['WANDB_API_KEY'] = user_secrets.get_secret('WANDB_API_KEY')\n",
        "else:\n",
        "  print(\"Please set the WANDB_API_KEY env variable manually\") #input()\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='GPT2-pretraining',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      'architecture': GPT2_variant,\n",
        "      'dataset': 'OpenWebText',\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'init_learning_rate': init_learning_rate,\n",
        "      'num_transformer_blocks': num_transformer_blocks,\n",
        "      'seqlen': seqlen,\n",
        "      'embed_dim': embed_dim,\n",
        "      'num_heads': num_heads,\n",
        "      'feed_forward_dim': feed_forward_dim,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'weight_decay': weight_decay\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "IbhEtsganEWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data\n",
        "\n",
        "Data loading and preprocessing with [Grain](https://github.com/google/grain)."
      ],
      "metadata": {
        "id": "mI1ci-HyMspJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  data_dir = \"/content/drive/MyDrive/LLM-pretraining/OpenWebText/\" # Assume dataset is already stored there\n",
        "elif platform == \"GCP\":\n",
        "  if not os.path.exists('OpenWebText-gpt2.zip'):\n",
        "    # Assume kaggle binary is in ~/.local/bin after pip install kaggle\n",
        "    !~/.local/bin/kaggle datasets download -d windmaple/OpenWebText-gpt2 && unzip OpenWebText-gpt2.zip\n",
        "  data_dir = \".\"\n",
        "elif platform == \"Kaggle\":  # On Kaggle one should manually add the datasets before running\n",
        "  data_dir = \"/kaggle/input/openwebtext-gpt2\"\n",
        "\n",
        "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "\n",
        "# From: https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/train.py#L116\n",
        "def get_batch(train_or_eval = \"train\"):\n",
        "\n",
        "    data = train_data if train_or_eval == \"train\" else val_data\n",
        "\n",
        "    ix = np.random.randint(0, len(data) - seqlen, (batch_size,))\n",
        "    x = np.stack([(data[i:i+seqlen]).astype(np.int64) for i in ix])\n",
        "    y = np.stack([(data[i+1:i+1+seqlen]).astype(np.int64) for i in ix])\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rGUFsn1GMuzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Define loss function and training step function."
      ],
      "metadata": {
        "id": "BKVSD8KSM1um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: nnx.Module, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ],
      "metadata": {
        "id": "8rRuTmABNV4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model and check the model parameter count."
      ],
      "metadata": {
        "id": "ZRRpiYBpwr2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "\n",
        "def count_params (params) :\n",
        "    p_sizes = jax.tree.map(lambda p: p.size if isinstance(p, jnp.ndarray) else 0, params)\n",
        "    import operator\n",
        "    return jax.tree.reduce(operator.add, p_sizes)\n",
        "\n",
        "print(f\"Number of model parameters: {count_params(nnx.state(model))}\")"
      ],
      "metadata": {
        "id": "D_L_PuTkwqtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally check the parameter count against the same model on Hugging Face."
      ],
      "metadata": {
        "id": "nBBJcz1B6XFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# from transformers import GPT2Model\n",
        "\n",
        "# # From https://github.com/huggingface/transformers/issues/27615\n",
        "# model = GPT2Model.from_pretrained('gpt2')\n",
        "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "# param_count = sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "# print(f\"Number of model parameters from Hugging Face: {param_count}\")"
      ],
      "metadata": {
        "id": "KDWyHtrU6S5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "5um2vkeUNckm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = optax.cosine_decay_schedule(\n",
        "  init_value=init_learning_rate,\n",
        "  decay_steps=max_steps\n",
        ")\n",
        "optax_chain = optax.chain(\n",
        "  optax.adamw(learning_rate=schedule, weight_decay=weight_decay)\n",
        ")\n",
        "optimizer = nnx.Optimizer(model, optax_chain)\n",
        "\n",
        "train_metrics = nnx.metrics.Average('loss')\n",
        "val_metrics = nnx.metrics.Average('val_loss')\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "print(f\"Inital generated text:\\n{start_prompt}\", end=\"\")\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'val_loss': []\n",
        "}\n",
        "\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    input_batch, target_batch = get_batch(\"train\")\n",
        "    if len(input_batch) % len(jax.devices()) != 0: continue  # skip the remaining elements\n",
        "    train_step(model, optimizer, train_metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
        "\n",
        "    if (step + 1) % 200 == 0:\n",
        "      train_loss = float(train_metrics.compute())\n",
        "      metrics_history['train_loss'].append(train_loss)\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(f\"Step {step + 1}, Training loss: {train_loss}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "      # eval step\n",
        "      input_val_batch, target_val_batch = get_batch('val')\n",
        "      loss, logits = loss_fn(model, jax.device_put((input_val_batch, target_val_batch), NamedSharding(mesh, P('batch', None))))\n",
        "      val_metrics.update(val_loss=loss, logits=logits)\n",
        "      val_loss = float(val_metrics.compute())\n",
        "      metrics_history['val_loss'].append(val_loss)\n",
        "      wandb.log(data={'val_loss': val_loss, 'train_loss': train_loss}, step=step)\n",
        "      print(f\"Step {step + 1}, Validation Loss: {val_loss}\")\n",
        "      train_metrics.reset()\n",
        "      val_metrics.reset()\n",
        "\n",
        "      start_time = time.time()\n",
        "    step += 1\n",
        "\n",
        "    if step > max_steps:\n",
        "      break\n",
        "\n",
        "# Final text generation\n",
        "print(f\"Final generated text:\\n{start_prompt}\", end=\"\")\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")"
      ],
      "metadata": {
        "id": "Ysl6CsfENeJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the training loss."
      ],
      "metadata": {
        "id": "thaLs6TD0lt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6Eg1Cz2y_iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible sentences at the end of the training.\n",
        "\n"
      ],
      "metadata": {
        "id": "26_HtTkZTczd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some training results ([wandb log](https://www.google.com/url?q=https%3A%2F%2Fwandb.ai%2Fwindmaple%2FGPT2-pretraining%3Fnw%3Dnwuserwindmaple)):\n",
        "\n",
        "| model | params | train loss | val loss | training time (TPU v3)\n",
        "| ------| ------ | ---------- | -------- | -----------------------\n",
        "| gpt2 | 124M         | 3.05  | 3.09     | 6.5 hr\n",
        "| gpt2-medium | 354M  | 2.83  | 2.86     | 22.5 hr\n",
        "\n",
        "The losses are roughly in line with [nanoGPT's](https://github.com/karpathy/nanoGPT?tab=readme-ov-file#baselines)."
      ],
      "metadata": {
        "id": "9hTE4MsEUh9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model saving"
      ],
      "metadata": {
        "id": "m13hUf7CcrSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import orbax.checkpoint as orbax\n",
        "import shutil\n",
        "\n",
        "if platform == \"Colab\":\n",
        "  checkpoint_path = \"/content/checkpoints\"\n",
        "elif platform == \"Kaggle\":\n",
        "  checkpoint_path = \"/kaggle/working/checkpoints\"\n",
        "else:\n",
        "  from pathlib import Path\n",
        "  home = Path.home()\n",
        "  checkpoint_path = os.path.join(str(home), \"checkpoints\")\n",
        "\n",
        "# make sure the folder is empty and usable\n",
        "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "train_state = nnx.state(model)\n",
        "checkpointer.save(checkpoint_path, train_state)"
      ],
      "metadata": {
        "id": "ofH_VkqMctZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model restoration\n",
        "Restore the model checkpoint. A pretrained checkpoint is also available [here](https://www.kaggle.com/models/windmaple/gpt2/)."
      ],
      "metadata": {
        "id": "soPqiR1JNmjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nnx.eval_shape(lambda: create_model(rngs=nnx.Rngs(0)))\n",
        "state = nnx.state(model)\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "state = checkpointer.restore(checkpoint_path, item=state)\n",
        "nnx.update(model, state)\n",
        "\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Restored model generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "EkoFGCgSZ1yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disconnect the Colab runtime"
      ],
      "metadata": {
        "id": "jCApVd7671c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import runtime\n",
        "  runtime.unassign()"
      ],
      "metadata": {
        "id": "NsqYdbrDVKSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}