{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 10577797,
          "sourceType": "datasetVersion",
          "datasetId": 5848741
        }
      ],
      "dockerImageVersionId": 30920,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrain the GPT2 model\n",
        "\n",
        "This notebook seeks to replicate Andrey Karpathy's highly popular [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) project and trains a GPT2 model from scratch using JAX+TPU and the [OpenWebText dataset](https://huggingface.co/datasets/Skylion007/openwebtext).\n",
        "\n",
        "You can run this on Colab, Kaggle or GCP TPUs, although Colab TPU v2 can only run the smallest 124M GPT model.\n"
      ],
      "metadata": {
        "id": "rvP1eNN_pExM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine platform"
      ],
      "metadata": {
        "id": "LD3bo9FxhrTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists('/content/'):\n",
        "  platform = \"Colab\"\n",
        "elif os.path.exists('/kaggle/'):\n",
        "  platform = \"Kaggle\"\n",
        "else:\n",
        "  # Assume using Cloud TPU otherwise\n",
        "  platform = \"GCP\""
      ],
      "metadata": {
        "id": "7XcEXnSbhhKV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:18.403068Z",
          "iopub.execute_input": "2025-02-21T04:30:18.403273Z",
          "iopub.status.idle": "2025-02-21T04:30:18.413321Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.403251Z",
          "shell.execute_reply": "2025-02-21T04:30:18.412689Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ],
      "metadata": {
        "id": "hTmz5Cbco7n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jax-ai-stack\n",
        "if platform == \"Colab\": # temp workaround on Colab (https://github.com/jax-ml/jax-ai-stack/issues/149)\n",
        "  !pip install -Uq \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info\n"
      ],
      "metadata": {
        "id": "6zMsOIc7ouCO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:18.414058Z",
          "iopub.execute_input": "2025-02-21T04:30:18.414249Z",
          "iopub.status.idle": "2025-02-21T04:30:51.718249Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.414229Z",
          "shell.execute_reply": "2025-02-21T04:30:51.716902Z"
        },
        "outputId": "5d9eb8b2-d95b-4a86-f3df-571ed936f4d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/99.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/424.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.2/424.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax-ai-stack 2025.2.5 requires jax==0.5.0, but you have jax 0.5.2 which is incompatible.\n",
            "tensorflow-tpu 2.18.0 requires libtpu==2.18.0, but you have libtpu 0.0.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.8/325.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm we have TPUs set up."
      ],
      "metadata": {
        "id": "6cWxBvz6bZDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "uZUaKdi5bSEN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:51.719166Z",
          "iopub.execute_input": "2025-02-21T04:30:51.719450Z",
          "iopub.status.idle": "2025-02-21T04:30:59.572796Z",
          "shell.execute_reply.started": "2025-02-21T04:30:51.719422Z",
          "shell.execute_reply": "2025-02-21T04:30:59.571540Z"
        },
        "outputId": "d859f559-b219-4681-d711-63b5c50ff6da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take care of the imports."
      ],
      "metadata": {
        "id": "sKE2uUafLobI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax, orbax\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import numpy as np\n",
        "import tiktoken, time, wandb"
      ],
      "metadata": {
        "id": "MKYFNOhdLq98",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:59.573640Z",
          "iopub.execute_input": "2025-02-21T04:30:59.573940Z",
          "iopub.status.idle": "2025-02-21T04:31:01.600418Z",
          "shell.execute_reply.started": "2025-02-21T04:30:59.573916Z",
          "shell.execute_reply": "2025-02-21T04:31:01.598633Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Define the device mesh.\n"
      ],
      "metadata": {
        "id": "rPyt7MV6prz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Alternative data and model parallel\n",
        "# mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
      ],
      "metadata": {
        "id": "xuMlCK3Q8WJD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:31:01.601360Z",
          "iopub.execute_input": "2025-02-21T04:31:01.601598Z",
          "iopub.status.idle": "2025-02-21T04:31:01.605772Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.601574Z",
          "shell.execute_reply": "2025-02-21T04:31:01.604615Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
      ],
      "metadata": {
        "id": "_ZKdhNo98NgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "iWbkk1V7-Isg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:31:01.606708Z",
          "iopub.execute_input": "2025-02-21T04:31:01.606937Z",
          "iopub.status.idle": "2025-02-21T04:31:04.402839Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.606915Z",
          "shell.execute_reply": "2025-02-21T04:31:04.401628Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set some hyperparameters."
      ],
      "metadata": {
        "id": "igX_eoGNMTGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "GPT2_variant = \"GPT2\" # \"GPT2-medium\"\n",
        "if GPT2_variant == \"GPT2-medium\":\n",
        "  num_transformer_blocks = 24\n",
        "  seqlen = 1024\n",
        "  embed_dim = 1024\n",
        "  num_heads = 16\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  batch_size = 32  # Can only run on TPU v3+\n",
        "else: ## Assume GPT2 otherwise\n",
        "  num_transformer_blocks = 12\n",
        "  seqlen = 1024\n",
        "  embed_dim = 768\n",
        "  num_heads = 12\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  if platform == \"Colab\":\n",
        "      batch_size = 24 # TPU v2\n",
        "  else:\n",
        "      batch_size = 72 # TPU v3\n",
        "\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_steps = 600000*12//batch_size\n",
        "# Kaggle TPU limit per session is 9 hours, which is ~95K steps for GPT2\n",
        "if platform == \"Kaggle\":\n",
        "  max_steps = 90000\n",
        "init_learning_rate = 5e-4\n",
        "weight_decay = 1e-1\n",
        "top_k = 10\n",
        "dtype = jnp.bfloat16\n",
        "param_dtype = jnp.float32"
      ],
      "metadata": {
        "id": "GRhiDsCrMZRp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:00.706531Z",
          "iopub.execute_input": "2025-02-21T04:32:00.706850Z",
          "iopub.status.idle": "2025-02-21T04:32:00.712524Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.706823Z",
          "shell.execute_reply": "2025-02-21T04:32:00.711567Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define the model architecture. You can refer to [OpenAI's official implementation](https://github.com/openai/gpt-2) or [nanoGPT](https://github.com/karpathy/nanoGPT) for comparison. Main difference is with the sharding scheme."
      ],
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float, rngs: nnx.Rngs):\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                          dtype=dtype,\n",
        "                                          param_dtype=param_dtype,\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            decode=False,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "\n",
        "        return x + mlp_output\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=seqlen, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding+position_embedding\n",
        "\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, num_heads: int, rate: float, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    seqlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.dropout = nnx.Dropout(rate=rate)\n",
        "\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.layer_norm = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                    num_features=embed_dim,\n",
        "                                    scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    dtype=dtype,\n",
        "                                    param_dtype=param_dtype,\n",
        "                                    rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        # Weights tying\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, padded_tokens, sample_index):\n",
        "        logits = self(padded_tokens)\n",
        "        next_token = self.sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        generated = []\n",
        "        for i in range(max_tokens):\n",
        "            sample_index = len(start_tokens) + len(generated) - 1\n",
        "            # TODO: use attention masking for better efficiency\n",
        "            padded_tokens = jnp.array((start_tokens + generated + [0] * (seqlen - len(start_tokens) - len(generated))))[None, :]\n",
        "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(next_token)\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return GPT2(seqlen, vocab_size, embed_dim, num_heads, dropout_rate, feed_forward_dim, num_transformer_blocks, rngs=rngs)"
      ],
      "metadata": {
        "id": "z0p-IHurrB9i",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:00.771150Z",
          "iopub.execute_input": "2025-02-21T04:32:00.771416Z",
          "iopub.status.idle": "2025-02-21T04:32:00.792958Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.771393Z",
          "shell.execute_reply": "2025-02-21T04:32:00.791974Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Weights and Biases to track training progress."
      ],
      "metadata": {
        "id": "eBfT1dp5hMUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import userdata\n",
        "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "  os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "  os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "elif platform == \"Kaggle\":\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  os.environ['WANDB_API_KEY'] = user_secrets.get_secret('WANDB_API_KEY')\n",
        "else:\n",
        "  print(\"Please set the WANDB_API_KEY env variable manually\") #input()\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='GPT2-pretraining',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      'architecture': GPT2_variant,\n",
        "      'dataset': 'OpenWebText',\n",
        "      'platform': platform,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'dtype': dtype,\n",
        "      'param_dtype': param_dtype,\n",
        "      'init_learning_rate': init_learning_rate,\n",
        "      'num_transformer_blocks': num_transformer_blocks,\n",
        "      'seqlen': seqlen,\n",
        "      'embed_dim': embed_dim,\n",
        "      'num_heads': num_heads,\n",
        "      'feed_forward_dim': feed_forward_dim,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'weight_decay': weight_decay\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "IbhEtsganEWg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:25.482072Z",
          "iopub.execute_input": "2025-02-21T04:32:25.482477Z",
          "iopub.status.idle": "2025-02-21T04:32:28.629414Z",
          "shell.execute_reply.started": "2025-02-21T04:32:25.482449Z",
          "shell.execute_reply": "2025-02-21T04:32:28.628576Z"
        },
        "outputId": "5a9352b5-8490-4d04-8ae5-d6b82e095f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindmaple\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250310_044206-8nibulkk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/windmaple/GPT2-pretraining/runs/8nibulkk' target=\"_blank\">lively-music-176</a></strong> to <a href='https://wandb.ai/windmaple/GPT2-pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/windmaple/GPT2-pretraining' target=\"_blank\">https://wandb.ai/windmaple/GPT2-pretraining</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/windmaple/GPT2-pretraining/runs/8nibulkk' target=\"_blank\">https://wandb.ai/windmaple/GPT2-pretraining/runs/8nibulkk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/windmaple/GPT2-pretraining/runs/8nibulkk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7da050798710>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data"
      ],
      "metadata": {
        "id": "mI1ci-HyMspJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  data_dir = \"/content/drive/MyDrive/LLM-pretraining/OpenWebText/\" # Assume dataset is already stored there\n",
        "elif platform == \"GCP\":\n",
        "  if not os.path.exists('OpenWebText-gpt2.zip'):\n",
        "    # Assume kaggle binary is in ~/.local/bin after pip install kaggle\n",
        "    !~/.local/bin/kaggle datasets download -d windmaple/OpenWebText-gpt2 && unzip OpenWebText-gpt2.zip\n",
        "  data_dir = \".\"\n",
        "elif platform == \"Kaggle\":  # On Kaggle one should manually add the datasets before running\n",
        "  data_dir = \"/kaggle/input/openwebtext-gpt2\"\n",
        "\n",
        "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "\n",
        "# From: https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/train.py#L116\n",
        "def get_batch(train_or_eval = \"train\"):\n",
        "\n",
        "    data = train_data if train_or_eval == \"train\" else val_data\n",
        "\n",
        "    ix = np.random.randint(0, len(data) - seqlen, (batch_size,))\n",
        "    x = np.stack([(data[i:i+seqlen]).astype(np.int64) for i in ix])\n",
        "    y = np.stack([(data[i+1:i+1+seqlen]).astype(np.int64) for i in ix])\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rGUFsn1GMuzh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:28.630426Z",
          "iopub.execute_input": "2025-02-21T04:32:28.630623Z",
          "iopub.status.idle": "2025-02-21T04:32:28.647808Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.630602Z",
          "shell.execute_reply": "2025-02-21T04:32:28.646734Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31c0ed4-89a9-4551-a892-818ab69ac252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Define loss function and training step function."
      ],
      "metadata": {
        "id": "BKVSD8KSM1um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: nnx.Module, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ],
      "metadata": {
        "id": "8rRuTmABNV4b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:28.648690Z",
          "iopub.execute_input": "2025-02-21T04:32:28.648880Z",
          "iopub.status.idle": "2025-02-21T04:32:28.683524Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.648860Z",
          "shell.execute_reply": "2025-02-21T04:32:28.682396Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model and check the model parameter count."
      ],
      "metadata": {
        "id": "ZRRpiYBpwr2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "\n",
        "p_sizes = jax.tree.map(lambda p: p.size if isinstance(p, jnp.ndarray) else 0, nnx.state(model))\n",
        "import operator\n",
        "print(f\"Number of model parameters: {jax.tree.reduce(operator.add, p_sizes)}\")"
      ],
      "metadata": {
        "id": "D_L_PuTkwqtu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:28.684258Z",
          "iopub.execute_input": "2025-02-21T04:32:28.684473Z",
          "iopub.status.idle": "2025-02-21T04:32:33.129248Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.684451Z",
          "shell.execute_reply": "2025-02-21T04:32:33.128221Z"
        },
        "outputId": "264b45d5-7907-4301-ff83-cd0d4ca6d963",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 124439808\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally check the parameter count against the same model on Hugging Face."
      ],
      "metadata": {
        "id": "nBBJcz1B6XFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# from transformers import GPT2Model\n",
        "\n",
        "# # From https://github.com/huggingface/transformers/issues/27615\n",
        "# model_hf = GPT2Model.from_pretrained('gpt2')\n",
        "# model_parameters = filter(lambda p: p.requires_grad, model_hf.parameters())\n",
        "# param_count = sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "# print(f\"Number of model parameters from Hugging Face: {param_count}\")"
      ],
      "metadata": {
        "id": "KDWyHtrU6S5f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:33.129850Z",
          "iopub.execute_input": "2025-02-21T04:32:33.130056Z",
          "iopub.status.idle": "2025-02-21T04:32:33.134982Z",
          "shell.execute_reply.started": "2025-02-21T04:32:33.130034Z",
          "shell.execute_reply": "2025-02-21T04:32:33.133692Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "5um2vkeUNckm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = optax.cosine_decay_schedule(\n",
        "  init_value=init_learning_rate,\n",
        "  decay_steps=max_steps\n",
        ")\n",
        "optax_chain = optax.chain(\n",
        "  optax.adamw(learning_rate=schedule, weight_decay=weight_decay)\n",
        ")\n",
        "optimizer = nnx.Optimizer(model, optax_chain)\n",
        "\n",
        "train_metrics = nnx.metrics.Average('loss')\n",
        "val_metrics = nnx.metrics.Average('val_loss')\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Initial generated text:\\n{generated_text}\")\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'val_loss': []\n",
        "}\n",
        "\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    input_batch, target_batch = get_batch(\"train\")\n",
        "    if len(input_batch) % len(jax.devices()) != 0: continue  # skip the remaining elements\n",
        "    train_step(model, optimizer, train_metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
        "\n",
        "    if step % 200 == 0:\n",
        "      train_loss = float(train_metrics.compute())\n",
        "      metrics_history['train_loss'].append(train_loss)\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(f\"Step {step + 1}, Training loss: {train_loss}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "      # eval step\n",
        "      input_val_batch, target_val_batch = get_batch('val')\n",
        "      loss, logits = loss_fn(model, jax.device_put((input_val_batch, target_val_batch), NamedSharding(mesh, P('batch', None))))\n",
        "      val_metrics.update(val_loss=loss, logits=logits)\n",
        "      val_loss = float(val_metrics.compute())\n",
        "      metrics_history['val_loss'].append(val_loss)\n",
        "      wandb.log(data={'val_loss': val_loss, 'train_loss': train_loss}, step=step)\n",
        "      print(f\"Step {step + 1}, Validation Loss: {val_loss}\")\n",
        "      train_metrics.reset()\n",
        "      val_metrics.reset()\n",
        "\n",
        "      start_time = time.time()\n",
        "    step += 1\n",
        "\n",
        "    if step > max_steps:\n",
        "      break\n",
        "\n",
        "# Final text generation\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Final generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "Ysl6CsfENeJN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:33.135918Z",
          "iopub.execute_input": "2025-02-21T04:32:33.136140Z",
          "iopub.status.idle": "2025-02-21T10:43:50.028572Z",
          "shell.execute_reply.started": "2025-02-21T04:32:33.136118Z",
          "shell.execute_reply": "2025-02-21T10:43:50.027504Z"
        },
        "outputId": "5fce194a-9133-4365-b5d2-22afb4f10966",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "Once upon a time secured PNG flattened principalsukuurances secureduku secureduku securedukuuku monopol Gothic monopoluku monopol Gothic Caucas Gothicuku Gothic Mel Caucas Mel Mel Mel monopol Caucasukuuku Caucas secured grizz grizz Caucas Melreditsilt responseively serotoninurred Gothic serotoninurred serotoninurred sewageusions Caucas Also Gothic sewageilt Caucasredits Caucas blockbuster blockbuster serotoninuku Xander defenses serotoninredits sewage Caucas Xander monopol Caucas Caucas Caucas grizz serotoniniltredits grizzilt grizz serotonin grizz Lutheranredits blockbusterredits grizz serotonin serotonin serotonin guarantee grizz Lutheran grizz Lutheran grizz Xanderredits Lutheran grizz Lutheran\n",
            "Step 1, Training loss: 11.5, Elapsed Time: 32.35 seconds\n",
            "Step 1, Validation Loss: 10.0\n",
            "Step 201, Training loss: 7.162656307220459, Elapsed Time: 237.45 seconds\n",
            "Step 201, Validation Loss: 6.71875\n",
            "Step 401, Training loss: 6.38406229019165, Elapsed Time: 41.62 seconds\n",
            "Step 401, Validation Loss: 6.125\n",
            "Step 601, Training loss: 6.160624980926514, Elapsed Time: 39.75 seconds\n",
            "Step 601, Validation Loss: 6.0\n",
            "Step 801, Training loss: 5.993593692779541, Elapsed Time: 38.99 seconds\n",
            "Step 801, Validation Loss: 5.84375\n",
            "Step 1001, Training loss: 5.820624828338623, Elapsed Time: 39.17 seconds\n",
            "Step 1001, Validation Loss: 5.71875\n",
            "Step 1201, Training loss: 5.667655944824219, Elapsed Time: 38.03 seconds\n",
            "Step 1201, Validation Loss: 5.625\n",
            "Step 1401, Training loss: 5.50906229019165, Elapsed Time: 39.10 seconds\n",
            "Step 1401, Validation Loss: 5.46875\n",
            "Step 1601, Training loss: 5.355781078338623, Elapsed Time: 38.86 seconds\n",
            "Step 1601, Validation Loss: 5.3125\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the training loss."
      ],
      "metadata": {
        "id": "thaLs6TD0lt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6Eg1Cz2y_iP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:43:50.029672Z",
          "iopub.execute_input": "2025-02-21T10:43:50.030074Z",
          "iopub.status.idle": "2025-02-21T10:43:51.584464Z",
          "shell.execute_reply.started": "2025-02-21T10:43:50.030047Z",
          "shell.execute_reply": "2025-02-21T10:43:51.583724Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible sentences at the end of the training.\n",
        "\n"
      ],
      "metadata": {
        "id": "26_HtTkZTczd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some training results on Kaggle TPU:\n",
        "\n",
        "| model | params | train loss | val loss | training time (TPU v3)\n",
        "| ------| ------ | ---------- | -------- | -----------------------\n",
        "| gpt2 | 124M         | 3.05  | 3.09     | 6.5 hr\n",
        "| gpt2-medium | 354M  | 2.83  | 2.86     | 22.5 hr\n",
        "\n",
        "The losses are roughly in line with [nanoGPT's](https://github.com/karpathy/nanoGPT?tab=readme-ov-file#baselines)."
      ],
      "metadata": {
        "id": "9hTE4MsEUh9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model saving"
      ],
      "metadata": {
        "id": "m13hUf7CcrSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import orbax.checkpoint as orbax\n",
        "import shutil\n",
        "\n",
        "if platform == \"Colab\":\n",
        "  checkpoint_path = \"/content/checkpoints\"\n",
        "elif platform == \"Kaggle\":\n",
        "  checkpoint_path = \"/kaggle/working/checkpoints\"\n",
        "else:\n",
        "  from pathlib import Path\n",
        "  home = Path.home()\n",
        "  checkpoint_path = os.path.join(str(home), \"checkpoints\")\n",
        "\n",
        "# make sure the folder is empty and usable\n",
        "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "train_state = nnx.state(model)\n",
        "checkpointer.save(checkpoint_path, train_state)"
      ],
      "metadata": {
        "id": "ofH_VkqMctZ5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:43:51.585651Z",
          "iopub.execute_input": "2025-02-21T10:43:51.586015Z",
          "iopub.status.idle": "2025-02-21T10:43:57.582123Z",
          "shell.execute_reply.started": "2025-02-21T10:43:51.585991Z",
          "shell.execute_reply": "2025-02-21T10:43:57.581319Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model restoration\n",
        "Restore the model checkpoint. A pretrained checkpoint is also available [here](https://www.kaggle.com/models/windmaple/gpt2/)."
      ],
      "metadata": {
        "id": "soPqiR1JNmjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nnx.eval_shape(lambda: create_model(rngs=nnx.Rngs(0)))\n",
        "state = nnx.state(model)\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "state = checkpointer.restore(checkpoint_path, item=state)\n",
        "nnx.update(model, state)\n",
        "\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Restored model generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "EkoFGCgSZ1yz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:43:57.583415Z",
          "iopub.execute_input": "2025-02-21T10:43:57.583673Z",
          "iopub.status.idle": "2025-02-21T10:44:12.704383Z",
          "shell.execute_reply.started": "2025-02-21T10:43:57.583648Z",
          "shell.execute_reply": "2025-02-21T10:44:12.703322Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disconnect the Colab runtime"
      ],
      "metadata": {
        "id": "jCApVd7671c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import runtime\n",
        "  runtime.unassign()"
      ],
      "metadata": {
        "id": "NsqYdbrDVKSq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:44:12.705516Z",
          "iopub.execute_input": "2025-02-21T10:44:12.705749Z",
          "iopub.status.idle": "2025-02-21T10:44:12.710493Z",
          "shell.execute_reply.started": "2025-02-21T10:44:12.705726Z",
          "shell.execute_reply": "2025-02-21T10:44:12.709284Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}