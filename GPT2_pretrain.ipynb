{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 10577797,
          "sourceType": "datasetVersion",
          "datasetId": 5848741
        }
      ],
      "dockerImageVersionId": 30920,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/windmaple/GPT2.JAX/blob/main/GPT2_pretrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrain the GPT2 model\n",
        "\n",
        "This notebook seeks to replicate Andrey Karpathy's highly popular [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) project and trains a GPT2 model from scratch using JAX+TPU and the [OpenWebText dataset](https://huggingface.co/datasets/Skylion007/openwebtext).\n",
        "\n",
        "You can run this on Colab, Kaggle or GCP TPUs, although Colab TPU v2 can only run the smallest 124M GPT model.\n"
      ],
      "metadata": {
        "id": "rvP1eNN_pExM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine platform"
      ],
      "metadata": {
        "id": "LD3bo9FxhrTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists('/content/'):\n",
        "  platform = \"Colab\"\n",
        "elif os.path.exists('/kaggle/'):\n",
        "  platform = \"Kaggle\"\n",
        "else:\n",
        "  # Assume using Cloud TPU otherwise\n",
        "  platform = \"GCP\""
      ],
      "metadata": {
        "id": "7XcEXnSbhhKV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:18.403068Z",
          "iopub.execute_input": "2025-02-21T04:30:18.403273Z",
          "iopub.status.idle": "2025-02-21T04:30:18.413321Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.403251Z",
          "shell.execute_reply": "2025-02-21T04:30:18.412689Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ],
      "metadata": {
        "id": "hTmz5Cbco7n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jax-ai-stack\n",
        "if platform == \"Colab\": # temp workaround on Colab (https://github.com/jax-ml/jax-ai-stack/issues/149)\n",
        "  !pip install -Uq \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info\n"
      ],
      "metadata": {
        "id": "6zMsOIc7ouCO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:18.414058Z",
          "iopub.execute_input": "2025-02-21T04:30:18.414249Z",
          "iopub.status.idle": "2025-02-21T04:30:51.718249Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.414229Z",
          "shell.execute_reply": "2025-02-21T04:30:51.716902Z"
        },
        "outputId": "99804d26-50c8-4ef1-8a92-bfecd9c83515"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm we have TPUs set up."
      ],
      "metadata": {
        "id": "6cWxBvz6bZDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "uZUaKdi5bSEN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:51.719166Z",
          "iopub.execute_input": "2025-02-21T04:30:51.719450Z",
          "iopub.status.idle": "2025-02-21T04:30:59.572796Z",
          "shell.execute_reply.started": "2025-02-21T04:30:51.719422Z",
          "shell.execute_reply": "2025-02-21T04:30:59.571540Z"
        },
        "outputId": "21be073a-e550-4ce6-b930-31be122ce95a"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1740112255.844336      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:230\n",
          "output_type": "stream"
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take care of the imports."
      ],
      "metadata": {
        "id": "sKE2uUafLobI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax, orbax\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import numpy as np\n",
        "import tiktoken, time, wandb"
      ],
      "metadata": {
        "id": "MKYFNOhdLq98",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:59.573640Z",
          "iopub.execute_input": "2025-02-21T04:30:59.573940Z",
          "iopub.status.idle": "2025-02-21T04:31:01.600418Z",
          "shell.execute_reply.started": "2025-02-21T04:30:59.573916Z",
          "shell.execute_reply": "2025-02-21T04:31:01.598633Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Define the device mesh.\n"
      ],
      "metadata": {
        "id": "rPyt7MV6prz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Alternative data and model parallel\n",
        "# mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
      ],
      "metadata": {
        "id": "xuMlCK3Q8WJD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:31:01.601360Z",
          "iopub.execute_input": "2025-02-21T04:31:01.601598Z",
          "iopub.status.idle": "2025-02-21T04:31:01.605772Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.601574Z",
          "shell.execute_reply": "2025-02-21T04:31:01.604615Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
      ],
      "metadata": {
        "id": "_ZKdhNo98NgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "iWbkk1V7-Isg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:31:01.606708Z",
          "iopub.execute_input": "2025-02-21T04:31:01.606937Z",
          "iopub.status.idle": "2025-02-21T04:31:04.402839Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.606915Z",
          "shell.execute_reply": "2025-02-21T04:31:04.401628Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set some hyperparameters."
      ],
      "metadata": {
        "id": "igX_eoGNMTGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "GPT2_variant = \"GPT2\" # \"GPT2-medium\"\n",
        "if GPT2_variant == \"GPT2-medium\":\n",
        "  num_transformer_blocks = 24\n",
        "  seqlen = 1024\n",
        "  embed_dim = 1024\n",
        "  num_heads = 16\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  batch_size = 32  # Can only run on TPU v3+\n",
        "else: ## Assume GPT2 otherwise\n",
        "  num_transformer_blocks = 12\n",
        "  seqlen = 1024\n",
        "  embed_dim = 768\n",
        "  num_heads = 12\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  if platform == \"Colab\":\n",
        "      batch_size = 24 # TPU v2\n",
        "  else:\n",
        "      batch_size = 72 # TPU v3\n",
        "\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_steps = 600000*12//batch_size\n",
        "# Kaggle TPU limit per session is 9 hours, which is ~95K steps for GPT2\n",
        "if platform == \"Kaggle\":\n",
        "  max_steps = 90000\n",
        "init_learning_rate = 5e-4\n",
        "weight_decay = 1e-1\n",
        "top_k = 10\n",
        "dtype = jnp.bfloat16\n",
        "param_dtype = jnp.float32"
      ],
      "metadata": {
        "id": "GRhiDsCrMZRp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:00.706531Z",
          "iopub.execute_input": "2025-02-21T04:32:00.706850Z",
          "iopub.status.idle": "2025-02-21T04:32:00.712524Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.706823Z",
          "shell.execute_reply": "2025-02-21T04:32:00.711567Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define the model architecture. You can refer to [OpenAI's official implementation](https://github.com/openai/gpt-2) or [nanoGPT](https://github.com/karpathy/nanoGPT) for comparison. Main difference is with the sharding scheme."
      ],
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float, rngs: nnx.Rngs):\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                          dtype=dtype,\n",
        "                                          param_dtype=param_dtype,\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            decode=False,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "\n",
        "        return x + mlp_output\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=seqlen, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding+position_embedding\n",
        "\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, num_heads: int, rate: float, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    seqlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.dropout = nnx.Dropout(rate=rate)\n",
        "\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.layer_norm = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                    num_features=embed_dim,\n",
        "                                    scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    dtype=dtype,\n",
        "                                    param_dtype=param_dtype,\n",
        "                                    rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        # Weights tying\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, padded_tokens, sample_index):\n",
        "        logits = self(padded_tokens)\n",
        "        next_token = self.sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        generated = []\n",
        "        for i in range(max_tokens):\n",
        "            sample_index = len(start_tokens) + len(generated) - 1\n",
        "            # TODO: use attention masking for better efficiency\n",
        "            padded_tokens = jnp.array((start_tokens + generated + [0] * (seqlen - len(start_tokens) - len(generated))))[None, :]\n",
        "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(next_token)\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return GPT2(seqlen, vocab_size, embed_dim, num_heads, dropout_rate, feed_forward_dim, num_transformer_blocks, rngs=rngs)"
      ],
      "metadata": {
        "id": "z0p-IHurrB9i",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:00.771150Z",
          "iopub.execute_input": "2025-02-21T04:32:00.771416Z",
          "iopub.status.idle": "2025-02-21T04:32:00.792958Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.771393Z",
          "shell.execute_reply": "2025-02-21T04:32:00.791974Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Weights and Biases to track training progress."
      ],
      "metadata": {
        "id": "eBfT1dp5hMUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import userdata\n",
        "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "  os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "  os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "elif platform == \"Kaggle\":\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  os.environ['WANDB_API_KEY'] = user_secrets.get_secret('WANDB_API_KEY')\n",
        "else:\n",
        "  print(\"Please set the WANDB_API_KEY env variable manually\") #input()\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='GPT2-pretraining',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      'architecture': GPT2_variant,\n",
        "      'dataset': 'OpenWebText',\n",
        "      'platform': platform,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'dtype': dtype,\n",
        "      'param_dtype': param_dtype,\n",
        "      'init_learning_rate': init_learning_rate,\n",
        "      'num_transformer_blocks': num_transformer_blocks,\n",
        "      'seqlen': seqlen,\n",
        "      'embed_dim': embed_dim,\n",
        "      'num_heads': num_heads,\n",
        "      'feed_forward_dim': feed_forward_dim,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'weight_decay': weight_decay\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "IbhEtsganEWg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:25.482072Z",
          "iopub.execute_input": "2025-02-21T04:32:25.482477Z",
          "iopub.status.idle": "2025-02-21T04:32:28.629414Z",
          "shell.execute_reply.started": "2025-02-21T04:32:25.482449Z",
          "shell.execute_reply": "2025-02-21T04:32:28.628576Z"
        },
        "outputId": "ddb55285-00a8-48b5-f1f8-5e24e26501f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindmaple\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.19.7"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20250221_043227-utdeztvj</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/windmaple/GPT2-pretraining/runs/utdeztvj' target=\"_blank\">super-valley-164</a></strong> to <a href='https://wandb.ai/windmaple/GPT2-pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/windmaple/GPT2-pretraining' target=\"_blank\">https://wandb.ai/windmaple/GPT2-pretraining</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/windmaple/GPT2-pretraining/runs/utdeztvj' target=\"_blank\">https://wandb.ai/windmaple/GPT2-pretraining/runs/utdeztvj</a>"
          },
          "metadata": {}
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/windmaple/GPT2-pretraining/runs/utdeztvj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
            "text/plain": "<wandb.sdk.wandb_run.Run at 0x7998dc385c90>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data\n",
        "\n",
        "Data loading and preprocessing with [Grain](https://github.com/google/grain)."
      ],
      "metadata": {
        "id": "mI1ci-HyMspJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  data_dir = \"/content/drive/MyDrive/LLM-pretraining/OpenWebText/\" # Assume dataset is already stored there\n",
        "elif platform == \"GCP\":\n",
        "  if not os.path.exists('OpenWebText-gpt2.zip'):\n",
        "    # Assume kaggle binary is in ~/.local/bin after pip install kaggle\n",
        "    !~/.local/bin/kaggle datasets download -d windmaple/OpenWebText-gpt2 && unzip OpenWebText-gpt2.zip\n",
        "  data_dir = \".\"\n",
        "elif platform == \"Kaggle\":  # On Kaggle one should manually add the datasets before running\n",
        "  data_dir = \"/kaggle/input/openwebtext-gpt2\"\n",
        "\n",
        "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "\n",
        "# From: https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/train.py#L116\n",
        "def get_batch(train_or_eval = \"train\"):\n",
        "\n",
        "    data = train_data if train_or_eval == \"train\" else val_data\n",
        "\n",
        "    ix = np.random.randint(0, len(data) - seqlen, (batch_size,))\n",
        "    x = np.stack([(data[i:i+seqlen]).astype(np.int64) for i in ix])\n",
        "    y = np.stack([(data[i+1:i+1+seqlen]).astype(np.int64) for i in ix])\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rGUFsn1GMuzh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:28.630426Z",
          "iopub.execute_input": "2025-02-21T04:32:28.630623Z",
          "iopub.status.idle": "2025-02-21T04:32:28.647808Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.630602Z",
          "shell.execute_reply": "2025-02-21T04:32:28.646734Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Define loss function and training step function."
      ],
      "metadata": {
        "id": "BKVSD8KSM1um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: nnx.Module, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ],
      "metadata": {
        "id": "8rRuTmABNV4b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:28.648690Z",
          "iopub.execute_input": "2025-02-21T04:32:28.648880Z",
          "iopub.status.idle": "2025-02-21T04:32:28.683524Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.648860Z",
          "shell.execute_reply": "2025-02-21T04:32:28.682396Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model and check the model parameter count."
      ],
      "metadata": {
        "id": "ZRRpiYBpwr2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "\n",
        "p_sizes = jax.tree.map(lambda p: p.size if isinstance(p, jnp.ndarray) else 0, nnx.state(model))\n",
        "import operator\n",
        "print(f\"Number of model parameters: {jax.tree.reduce(operator.add, p_sizes)}\")"
      ],
      "metadata": {
        "id": "D_L_PuTkwqtu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:28.684258Z",
          "iopub.execute_input": "2025-02-21T04:32:28.684473Z",
          "iopub.status.idle": "2025-02-21T04:32:33.129248Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.684451Z",
          "shell.execute_reply": "2025-02-21T04:32:33.128221Z"
        },
        "outputId": "7ae0c069-8d2b-4eb9-b7f0-479d53c5488c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of model parameters: 124439808\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally check the parameter count against the same model on Hugging Face."
      ],
      "metadata": {
        "id": "nBBJcz1B6XFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# from transformers import GPT2Model\n",
        "\n",
        "# # From https://github.com/huggingface/transformers/issues/27615\n",
        "# model = GPT2Model.from_pretrained('gpt2')\n",
        "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "# param_count = sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "# print(f\"Number of model parameters from Hugging Face: {param_count}\")"
      ],
      "metadata": {
        "id": "KDWyHtrU6S5f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:33.129850Z",
          "iopub.execute_input": "2025-02-21T04:32:33.130056Z",
          "iopub.status.idle": "2025-02-21T04:32:33.134982Z",
          "shell.execute_reply.started": "2025-02-21T04:32:33.130034Z",
          "shell.execute_reply": "2025-02-21T04:32:33.133692Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "5um2vkeUNckm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = optax.cosine_decay_schedule(\n",
        "  init_value=init_learning_rate,\n",
        "  decay_steps=max_steps\n",
        ")\n",
        "optax_chain = optax.chain(\n",
        "  optax.adamw(learning_rate=schedule, weight_decay=weight_decay)\n",
        ")\n",
        "optimizer = nnx.Optimizer(model, optax_chain)\n",
        "\n",
        "train_metrics = nnx.metrics.Average('loss')\n",
        "val_metrics = nnx.metrics.Average('val_loss')\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Initial generated text:\\n{generated_text}\")\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'val_loss': []\n",
        "}\n",
        "\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    input_batch, target_batch = get_batch(\"train\")\n",
        "    if len(input_batch) % len(jax.devices()) != 0: continue  # skip the remaining elements\n",
        "    train_step(model, optimizer, train_metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
        "\n",
        "    if (step + 1) % 200 == 0:\n",
        "      train_loss = float(train_metrics.compute())\n",
        "      metrics_history['train_loss'].append(train_loss)\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(f\"Step {step + 1}, Training loss: {train_loss}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "      # eval step\n",
        "      input_val_batch, target_val_batch = get_batch('val')\n",
        "      loss, logits = loss_fn(model, jax.device_put((input_val_batch, target_val_batch), NamedSharding(mesh, P('batch', None))))\n",
        "      val_metrics.update(val_loss=loss, logits=logits)\n",
        "      val_loss = float(val_metrics.compute())\n",
        "      metrics_history['val_loss'].append(val_loss)\n",
        "      wandb.log(data={'val_loss': val_loss, 'train_loss': train_loss}, step=step)\n",
        "      print(f\"Step {step + 1}, Validation Loss: {val_loss}\")\n",
        "      train_metrics.reset()\n",
        "      val_metrics.reset()\n",
        "\n",
        "      start_time = time.time()\n",
        "    step += 1\n",
        "\n",
        "    if step > max_steps:\n",
        "      break\n",
        "\n",
        "# Final text generation\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Final generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "Ysl6CsfENeJN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:33.135918Z",
          "iopub.execute_input": "2025-02-21T04:32:33.136140Z",
          "iopub.status.idle": "2025-02-21T10:43:50.028572Z",
          "shell.execute_reply.started": "2025-02-21T04:32:33.136118Z",
          "shell.execute_reply": "2025-02-21T10:43:50.027504Z"
        },
        "outputId": "9a9fe90f-0069-422e-9a18-6a96bdbdea5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Inital generated text:\nOnce upon a timeStep 200, Training loss: 7.184999942779541, Elapsed Time: 119.87 seconds\nStep 200, Validation Loss: 6.5\nStep 400, Training loss: 6.241250038146973, Elapsed Time: 87.31 seconds\nStep 400, Validation Loss: 6.125\nStep 600, Training loss: 5.920781135559082, Elapsed Time: 56.97 seconds\nStep 600, Validation Loss: 5.78125\nStep 800, Training loss: 5.629843711853027, Elapsed Time: 52.34 seconds\nStep 800, Validation Loss: 5.46875\nStep 1000, Training loss: 5.355155944824219, Elapsed Time: 48.99 seconds\nStep 1000, Validation Loss: 5.25\nStep 1200, Training loss: 5.118593692779541, Elapsed Time: 49.04 seconds\nStep 1200, Validation Loss: 5.0\nStep 1400, Training loss: 4.920937538146973, Elapsed Time: 49.14 seconds\nStep 1400, Validation Loss: 4.84375\nStep 1600, Training loss: 4.720312595367432, Elapsed Time: 49.12 seconds\nStep 1600, Validation Loss: 4.625\nStep 1800, Training loss: 4.556406021118164, Elapsed Time: 49.11 seconds\nStep 1800, Validation Loss: 4.4375\nStep 2000, Training loss: 4.443593502044678, Elapsed Time: 49.09 seconds\nStep 2000, Validation Loss: 4.375\nStep 2200, Training loss: 4.354374885559082, Elapsed Time: 49.09 seconds\nStep 2200, Validation Loss: 4.25\nStep 2400, Training loss: 4.28640604019165, Elapsed Time: 49.07 seconds\nStep 2400, Validation Loss: 4.21875\nStep 2600, Training loss: 4.212812423706055, Elapsed Time: 49.10 seconds\nStep 2600, Validation Loss: 4.15625\nStep 2800, Training loss: 4.169374942779541, Elapsed Time: 49.09 seconds\nStep 2800, Validation Loss: 4.125\nStep 3000, Training loss: 4.112734317779541, Elapsed Time: 49.06 seconds\nStep 3000, Validation Loss: 4.09375\nStep 3200, Training loss: 4.069296836853027, Elapsed Time: 49.06 seconds\nStep 3200, Validation Loss: 4.15625\nStep 3400, Training loss: 4.036015510559082, Elapsed Time: 49.07 seconds\nStep 3400, Validation Loss: 3.984375\nStep 3600, Training loss: 4.003046989440918, Elapsed Time: 49.05 seconds\nStep 3600, Validation Loss: 3.96875\nStep 3800, Training loss: 3.968827962875366, Elapsed Time: 49.05 seconds\nStep 3800, Validation Loss: 4.0\nStep 4000, Training loss: 3.9399218559265137, Elapsed Time: 49.04 seconds\nStep 4000, Validation Loss: 3.890625\nStep 4200, Training loss: 3.914374828338623, Elapsed Time: 49.04 seconds\nStep 4200, Validation Loss: 3.84375\nStep 4400, Training loss: 3.8871092796325684, Elapsed Time: 49.04 seconds\nStep 4400, Validation Loss: 3.90625\nStep 4600, Training loss: 3.865000009536743, Elapsed Time: 49.05 seconds\nStep 4600, Validation Loss: 3.890625\nStep 4800, Training loss: 3.8436717987060547, Elapsed Time: 49.03 seconds\nStep 4800, Validation Loss: 3.8125\nStep 5000, Training loss: 3.822499990463257, Elapsed Time: 49.03 seconds\nStep 5000, Validation Loss: 3.828125\nStep 5200, Training loss: 3.8017187118530273, Elapsed Time: 49.04 seconds\nStep 5200, Validation Loss: 3.875\nStep 5400, Training loss: 3.7845311164855957, Elapsed Time: 49.03 seconds\nStep 5400, Validation Loss: 3.75\nStep 5600, Training loss: 3.7777342796325684, Elapsed Time: 49.03 seconds\nStep 5600, Validation Loss: 3.8125\nStep 5800, Training loss: 3.758906126022339, Elapsed Time: 49.03 seconds\nStep 5800, Validation Loss: 3.765625\nStep 6000, Training loss: 3.738515615463257, Elapsed Time: 49.05 seconds\nStep 6000, Validation Loss: 3.71875\nStep 6200, Training loss: 3.7298436164855957, Elapsed Time: 49.02 seconds\nStep 6200, Validation Loss: 3.703125\nStep 6400, Training loss: 3.7174999713897705, Elapsed Time: 49.02 seconds\nStep 6400, Validation Loss: 3.703125\nStep 6600, Training loss: 3.7074999809265137, Elapsed Time: 49.03 seconds\nStep 6600, Validation Loss: 3.75\nStep 6800, Training loss: 3.70703125, Elapsed Time: 49.04 seconds\nStep 6800, Validation Loss: 3.734375\nStep 7000, Training loss: 3.6860156059265137, Elapsed Time: 49.02 seconds\nStep 7000, Validation Loss: 3.625\nStep 7200, Training loss: 3.6703906059265137, Elapsed Time: 49.02 seconds\nStep 7200, Validation Loss: 3.671875\nStep 7400, Training loss: 3.655390501022339, Elapsed Time: 49.01 seconds\nStep 7400, Validation Loss: 3.6875\nStep 7600, Training loss: 3.6514060497283936, Elapsed Time: 49.03 seconds\nStep 7600, Validation Loss: 3.640625\nStep 7800, Training loss: 3.6464061737060547, Elapsed Time: 49.02 seconds\nStep 7800, Validation Loss: 3.625\nStep 8000, Training loss: 3.6357030868530273, Elapsed Time: 49.05 seconds\nStep 8000, Validation Loss: 3.609375\nStep 8200, Training loss: 3.6238279342651367, Elapsed Time: 49.01 seconds\nStep 8200, Validation Loss: 3.640625\nStep 8400, Training loss: 3.620859384536743, Elapsed Time: 49.01 seconds\nStep 8400, Validation Loss: 3.625\nStep 8600, Training loss: 3.6048436164855957, Elapsed Time: 49.01 seconds\nStep 8600, Validation Loss: 3.65625\nStep 8800, Training loss: 3.595781087875366, Elapsed Time: 49.01 seconds\nStep 8800, Validation Loss: 3.609375\nStep 9000, Training loss: 3.5964841842651367, Elapsed Time: 49.02 seconds\nStep 9000, Validation Loss: 3.5625\nStep 9200, Training loss: 3.5810937881469727, Elapsed Time: 49.01 seconds\nStep 9200, Validation Loss: 3.578125\nStep 9400, Training loss: 3.5855467319488525, Elapsed Time: 49.02 seconds\nStep 9400, Validation Loss: 3.59375\nStep 9600, Training loss: 3.5715625286102295, Elapsed Time: 49.02 seconds\nStep 9600, Validation Loss: 3.609375\nStep 9800, Training loss: 3.561718702316284, Elapsed Time: 49.01 seconds\nStep 9800, Validation Loss: 3.640625\nStep 10000, Training loss: 3.5626561641693115, Elapsed Time: 49.01 seconds\nStep 10000, Validation Loss: 3.578125\nStep 10200, Training loss: 3.552421808242798, Elapsed Time: 49.01 seconds\nStep 10200, Validation Loss: 3.546875\nStep 10400, Training loss: 3.551015615463257, Elapsed Time: 49.01 seconds\nStep 10400, Validation Loss: 3.546875\nStep 10600, Training loss: 3.5398435592651367, Elapsed Time: 49.02 seconds\nStep 10600, Validation Loss: 3.53125\nStep 10800, Training loss: 3.5294530391693115, Elapsed Time: 49.01 seconds\nStep 10800, Validation Loss: 3.71875\nStep 11000, Training loss: 3.531015634536743, Elapsed Time: 49.01 seconds\nStep 11000, Validation Loss: 3.53125\nStep 11200, Training loss: 3.5310935974121094, Elapsed Time: 49.01 seconds\nStep 11200, Validation Loss: 3.5\nStep 11400, Training loss: 3.5188281536102295, Elapsed Time: 49.00 seconds\nStep 11400, Validation Loss: 3.5\nStep 11600, Training loss: 3.517031192779541, Elapsed Time: 49.02 seconds\nStep 11600, Validation Loss: 3.53125\nStep 11800, Training loss: 3.5037498474121094, Elapsed Time: 49.01 seconds\nStep 11800, Validation Loss: 3.515625\nStep 12000, Training loss: 3.5088281631469727, Elapsed Time: 49.01 seconds\nStep 12000, Validation Loss: 3.5\nStep 12200, Training loss: 3.5063281059265137, Elapsed Time: 49.00 seconds\nStep 12200, Validation Loss: 3.515625\nStep 12400, Training loss: 3.4934375286102295, Elapsed Time: 49.01 seconds\nStep 12400, Validation Loss: 3.453125\nStep 12600, Training loss: 3.4898436069488525, Elapsed Time: 49.03 seconds\nStep 12600, Validation Loss: 3.546875\nStep 12800, Training loss: 3.4954686164855957, Elapsed Time: 49.02 seconds\nStep 12800, Validation Loss: 3.546875\nStep 13000, Training loss: 3.479140520095825, Elapsed Time: 49.02 seconds\nStep 13000, Validation Loss: 3.5\nStep 13200, Training loss: 3.480234384536743, Elapsed Time: 49.00 seconds\nStep 13200, Validation Loss: 3.515625\nStep 13400, Training loss: 3.480781078338623, Elapsed Time: 49.00 seconds\nStep 13400, Validation Loss: 3.515625\nStep 13600, Training loss: 3.4755468368530273, Elapsed Time: 49.01 seconds\nStep 13600, Validation Loss: 3.484375\nStep 13800, Training loss: 3.474375009536743, Elapsed Time: 49.02 seconds\nStep 13800, Validation Loss: 3.421875\nStep 14000, Training loss: 3.4681248664855957, Elapsed Time: 49.01 seconds\nStep 14000, Validation Loss: 3.5\nStep 14200, Training loss: 3.461015462875366, Elapsed Time: 49.00 seconds\nStep 14200, Validation Loss: 3.390625\nStep 14400, Training loss: 3.454765558242798, Elapsed Time: 49.01 seconds\nStep 14400, Validation Loss: 3.5\nStep 14600, Training loss: 3.456171751022339, Elapsed Time: 49.00 seconds\nStep 14600, Validation Loss: 3.453125\nStep 14800, Training loss: 3.458124876022339, Elapsed Time: 49.00 seconds\nStep 14800, Validation Loss: 3.453125\nStep 15000, Training loss: 3.4538280963897705, Elapsed Time: 49.01 seconds\nStep 15000, Validation Loss: 3.4375\nStep 15200, Training loss: 3.4443750381469727, Elapsed Time: 49.01 seconds\nStep 15200, Validation Loss: 3.375\nStep 15400, Training loss: 3.4465625286102295, Elapsed Time: 49.00 seconds\nStep 15400, Validation Loss: 3.546875\nStep 15600, Training loss: 3.437577962875366, Elapsed Time: 49.00 seconds\nStep 15600, Validation Loss: 3.453125\nStep 15800, Training loss: 3.440546751022339, Elapsed Time: 49.00 seconds\nStep 15800, Validation Loss: 3.515625\nStep 16000, Training loss: 3.4296875, Elapsed Time: 49.01 seconds\nStep 16000, Validation Loss: 3.453125\nStep 16200, Training loss: 3.4311718940734863, Elapsed Time: 49.00 seconds\nStep 16200, Validation Loss: 3.484375\nStep 16400, Training loss: 3.427968740463257, Elapsed Time: 49.01 seconds\nStep 16400, Validation Loss: 3.421875\nStep 16600, Training loss: 3.423281192779541, Elapsed Time: 49.01 seconds\nStep 16600, Validation Loss: 3.34375\nStep 16800, Training loss: 3.423281192779541, Elapsed Time: 49.00 seconds\nStep 16800, Validation Loss: 3.40625\nStep 17000, Training loss: 3.4170312881469727, Elapsed Time: 49.00 seconds\nStep 17000, Validation Loss: 3.3125\nStep 17200, Training loss: 3.4169530868530273, Elapsed Time: 49.00 seconds\nStep 17200, Validation Loss: 3.40625\nStep 17400, Training loss: 3.41460919380188, Elapsed Time: 49.00 seconds\nStep 17400, Validation Loss: 3.515625\nStep 17600, Training loss: 3.4097654819488525, Elapsed Time: 49.00 seconds\nStep 17600, Validation Loss: 3.375\nStep 17800, Training loss: 3.402109384536743, Elapsed Time: 49.01 seconds\nStep 17800, Validation Loss: 3.421875\nStep 18000, Training loss: 3.407421827316284, Elapsed Time: 49.01 seconds\nStep 18000, Validation Loss: 3.375\nStep 18200, Training loss: 3.406015634536743, Elapsed Time: 49.01 seconds\nStep 18200, Validation Loss: 3.421875\nStep 18400, Training loss: 3.4011716842651367, Elapsed Time: 49.00 seconds\nStep 18400, Validation Loss: 3.375\nStep 18600, Training loss: 3.398515462875366, Elapsed Time: 49.00 seconds\nStep 18600, Validation Loss: 3.328125\nStep 18800, Training loss: 3.394218683242798, Elapsed Time: 49.01 seconds\nStep 18800, Validation Loss: 3.390625\nStep 19000, Training loss: 3.392812490463257, Elapsed Time: 49.00 seconds\nStep 19000, Validation Loss: 3.328125\nStep 19200, Training loss: 3.3910155296325684, Elapsed Time: 49.00 seconds\nStep 19200, Validation Loss: 3.5\nStep 19400, Training loss: 3.39507794380188, Elapsed Time: 49.00 seconds\nStep 19400, Validation Loss: 3.390625\nStep 19600, Training loss: 3.389531135559082, Elapsed Time: 49.00 seconds\nStep 19600, Validation Loss: 3.421875\nStep 19800, Training loss: 3.3871092796325684, Elapsed Time: 49.00 seconds\nStep 19800, Validation Loss: 3.3125\nStep 20000, Training loss: 3.3807811737060547, Elapsed Time: 49.00 seconds\nStep 20000, Validation Loss: 3.390625\nStep 20200, Training loss: 3.37749981880188, Elapsed Time: 49.00 seconds\nStep 20200, Validation Loss: 3.359375\nStep 20400, Training loss: 3.376171827316284, Elapsed Time: 49.00 seconds\nStep 20400, Validation Loss: 3.375\nStep 20600, Training loss: 3.3738279342651367, Elapsed Time: 49.01 seconds\nStep 20600, Validation Loss: 3.390625\nStep 20800, Training loss: 3.3710155487060547, Elapsed Time: 49.00 seconds\nStep 20800, Validation Loss: 3.359375\nStep 21000, Training loss: 3.374140501022339, Elapsed Time: 49.00 seconds\nStep 21000, Validation Loss: 3.359375\nStep 21200, Training loss: 3.3767967224121094, Elapsed Time: 49.01 seconds\nStep 21200, Validation Loss: 3.421875\nStep 21400, Training loss: 3.369218587875366, Elapsed Time: 49.00 seconds\nStep 21400, Validation Loss: 3.4375\nStep 21600, Training loss: 3.367265462875366, Elapsed Time: 49.01 seconds\nStep 21600, Validation Loss: 3.484375\nStep 21800, Training loss: 3.3645312786102295, Elapsed Time: 49.01 seconds\nStep 21800, Validation Loss: 3.46875\nStep 22000, Training loss: 3.3628125190734863, Elapsed Time: 49.00 seconds\nStep 22000, Validation Loss: 3.421875\nStep 22200, Training loss: 3.3559374809265137, Elapsed Time: 49.01 seconds\nStep 22200, Validation Loss: 3.3125\nStep 22400, Training loss: 3.357734203338623, Elapsed Time: 49.00 seconds\nStep 22400, Validation Loss: 3.359375\nStep 22600, Training loss: 3.349843740463257, Elapsed Time: 49.01 seconds\nStep 22600, Validation Loss: 3.328125\nStep 22800, Training loss: 3.3583593368530273, Elapsed Time: 49.00 seconds\nStep 22800, Validation Loss: 3.4375\nStep 23000, Training loss: 3.354609251022339, Elapsed Time: 49.00 seconds\nStep 23000, Validation Loss: 3.328125\nStep 23200, Training loss: 3.3464841842651367, Elapsed Time: 49.00 seconds\nStep 23200, Validation Loss: 3.375\nStep 23400, Training loss: 3.349296808242798, Elapsed Time: 49.00 seconds\nStep 23400, Validation Loss: 3.4375\nStep 23600, Training loss: 3.3478124141693115, Elapsed Time: 49.00 seconds\nStep 23600, Validation Loss: 3.421875\nStep 23800, Training loss: 3.353749990463257, Elapsed Time: 49.00 seconds\nStep 23800, Validation Loss: 3.328125\nStep 24000, Training loss: 3.3464062213897705, Elapsed Time: 49.00 seconds\nStep 24000, Validation Loss: 3.296875\nStep 24200, Training loss: 3.339609384536743, Elapsed Time: 49.00 seconds\nStep 24200, Validation Loss: 3.34375\nStep 24400, Training loss: 3.342968702316284, Elapsed Time: 49.00 seconds\nStep 24400, Validation Loss: 3.390625\nStep 24600, Training loss: 3.338515520095825, Elapsed Time: 49.00 seconds\nStep 24600, Validation Loss: 3.3125\nStep 24800, Training loss: 3.335703134536743, Elapsed Time: 49.01 seconds\nStep 24800, Validation Loss: 3.328125\nStep 25000, Training loss: 3.3428125381469727, Elapsed Time: 49.00 seconds\nStep 25000, Validation Loss: 3.359375\nStep 25200, Training loss: 3.335703134536743, Elapsed Time: 49.01 seconds\nStep 25200, Validation Loss: 3.3125\nStep 25400, Training loss: 3.330312490463257, Elapsed Time: 49.00 seconds\nStep 25400, Validation Loss: 3.296875\nStep 25600, Training loss: 3.3333592414855957, Elapsed Time: 49.01 seconds\nStep 25600, Validation Loss: 3.34375\nStep 25800, Training loss: 3.325937509536743, Elapsed Time: 49.01 seconds\nStep 25800, Validation Loss: 3.375\nStep 26000, Training loss: 3.3302342891693115, Elapsed Time: 49.00 seconds\nStep 26000, Validation Loss: 3.328125\nStep 26200, Training loss: 3.327343702316284, Elapsed Time: 49.00 seconds\nStep 26200, Validation Loss: 3.3125\nStep 26400, Training loss: 3.3191404342651367, Elapsed Time: 49.00 seconds\nStep 26400, Validation Loss: 3.265625\nStep 26600, Training loss: 3.327890634536743, Elapsed Time: 49.00 seconds\nStep 26600, Validation Loss: 3.359375\nStep 26800, Training loss: 3.3201560974121094, Elapsed Time: 49.00 seconds\nStep 26800, Validation Loss: 3.328125\nStep 27000, Training loss: 3.3230466842651367, Elapsed Time: 49.00 seconds\nStep 27000, Validation Loss: 3.3125\nStep 27200, Training loss: 3.322343587875366, Elapsed Time: 49.01 seconds\nStep 27200, Validation Loss: 3.390625\nStep 27400, Training loss: 3.3152341842651367, Elapsed Time: 49.00 seconds\nStep 27400, Validation Loss: 3.265625\nStep 27600, Training loss: 3.3157811164855957, Elapsed Time: 49.00 seconds\nStep 27600, Validation Loss: 3.296875\nStep 27800, Training loss: 3.3135156631469727, Elapsed Time: 49.00 seconds\nStep 27800, Validation Loss: 3.3125\nStep 28000, Training loss: 3.319453001022339, Elapsed Time: 49.00 seconds\nStep 28000, Validation Loss: 3.28125\nStep 28200, Training loss: 3.3207812309265137, Elapsed Time: 49.00 seconds\nStep 28200, Validation Loss: 3.34375\nStep 28400, Training loss: 3.310781240463257, Elapsed Time: 49.00 seconds\nStep 28400, Validation Loss: 3.3125\nStep 28600, Training loss: 3.306718587875366, Elapsed Time: 49.00 seconds\nStep 28600, Validation Loss: 3.296875\nStep 28800, Training loss: 3.309218645095825, Elapsed Time: 49.00 seconds\nStep 28800, Validation Loss: 3.3125\nStep 29000, Training loss: 3.310624837875366, Elapsed Time: 49.00 seconds\nStep 29000, Validation Loss: 3.421875\nStep 29200, Training loss: 3.305312395095825, Elapsed Time: 49.00 seconds\nStep 29200, Validation Loss: 3.34375\nStep 29400, Training loss: 3.3071093559265137, Elapsed Time: 49.00 seconds\nStep 29400, Validation Loss: 3.34375\nStep 29600, Training loss: 3.296562433242798, Elapsed Time: 49.02 seconds\nStep 29600, Validation Loss: 3.328125\nStep 29800, Training loss: 3.2982029914855957, Elapsed Time: 49.02 seconds\nStep 29800, Validation Loss: 3.34375\nStep 30000, Training loss: 3.2997655868530273, Elapsed Time: 49.00 seconds\nStep 30000, Validation Loss: 3.328125\nStep 30200, Training loss: 3.2992186546325684, Elapsed Time: 49.00 seconds\nStep 30200, Validation Loss: 3.3125\nStep 30400, Training loss: 3.297499895095825, Elapsed Time: 49.00 seconds\nStep 30400, Validation Loss: 3.359375\nStep 30600, Training loss: 3.2948436737060547, Elapsed Time: 49.01 seconds\nStep 30600, Validation Loss: 3.296875\nStep 30800, Training loss: 3.294999837875366, Elapsed Time: 49.01 seconds\nStep 30800, Validation Loss: 3.265625\nStep 31000, Training loss: 3.2937498092651367, Elapsed Time: 49.00 seconds\nStep 31000, Validation Loss: 3.34375\nStep 31200, Training loss: 3.2942187786102295, Elapsed Time: 49.01 seconds\nStep 31200, Validation Loss: 3.296875\nStep 31400, Training loss: 3.293281078338623, Elapsed Time: 49.01 seconds\nStep 31400, Validation Loss: 3.421875\nStep 31600, Training loss: 3.2899999618530273, Elapsed Time: 49.01 seconds\nStep 31600, Validation Loss: 3.34375\nStep 31800, Training loss: 3.287187337875366, Elapsed Time: 49.00 seconds\nStep 31800, Validation Loss: 3.296875\nStep 32000, Training loss: 3.2853124141693115, Elapsed Time: 49.01 seconds\nStep 32000, Validation Loss: 3.234375\nStep 32200, Training loss: 3.2885937690734863, Elapsed Time: 49.00 seconds\nStep 32200, Validation Loss: 3.359375\nStep 32400, Training loss: 3.279609203338623, Elapsed Time: 49.02 seconds\nStep 32400, Validation Loss: 3.25\nStep 32600, Training loss: 3.281015634536743, Elapsed Time: 49.01 seconds\nStep 32600, Validation Loss: 3.328125\nStep 32800, Training loss: 3.2814061641693115, Elapsed Time: 49.01 seconds\nStep 32800, Validation Loss: 3.34375\nStep 33000, Training loss: 3.2792186737060547, Elapsed Time: 49.00 seconds\nStep 33000, Validation Loss: 3.28125\nStep 33200, Training loss: 3.280390501022339, Elapsed Time: 49.00 seconds\nStep 33200, Validation Loss: 3.3125\nStep 33400, Training loss: 3.281484365463257, Elapsed Time: 49.00 seconds\nStep 33400, Validation Loss: 3.28125\nStep 33600, Training loss: 3.2724218368530273, Elapsed Time: 49.01 seconds\nStep 33600, Validation Loss: 3.3125\nStep 33800, Training loss: 3.2805469036102295, Elapsed Time: 49.01 seconds\nStep 33800, Validation Loss: 3.265625\nStep 34000, Training loss: 3.2733592987060547, Elapsed Time: 49.00 seconds\nStep 34000, Validation Loss: 3.21875\nStep 34200, Training loss: 3.274609327316284, Elapsed Time: 49.00 seconds\nStep 34200, Validation Loss: 3.265625\nStep 34400, Training loss: 3.269218683242798, Elapsed Time: 49.00 seconds\nStep 34400, Validation Loss: 3.359375\nStep 34600, Training loss: 3.26812481880188, Elapsed Time: 49.02 seconds\nStep 34600, Validation Loss: 3.296875\nStep 34800, Training loss: 3.267031192779541, Elapsed Time: 49.00 seconds\nStep 34800, Validation Loss: 3.28125\nStep 35000, Training loss: 3.2616405487060547, Elapsed Time: 49.01 seconds\nStep 35000, Validation Loss: 3.21875\nStep 35200, Training loss: 3.2660937309265137, Elapsed Time: 49.00 seconds\nStep 35200, Validation Loss: 3.34375\nStep 35400, Training loss: 3.2699999809265137, Elapsed Time: 49.00 seconds\nStep 35400, Validation Loss: 3.3125\nStep 35600, Training loss: 3.26031231880188, Elapsed Time: 49.01 seconds\nStep 35600, Validation Loss: 3.171875\nStep 35800, Training loss: 3.259218692779541, Elapsed Time: 49.00 seconds\nStep 35800, Validation Loss: 3.234375\nStep 36000, Training loss: 3.2593748569488525, Elapsed Time: 49.00 seconds\nStep 36000, Validation Loss: 3.171875\nStep 36200, Training loss: 3.2646093368530273, Elapsed Time: 49.01 seconds\nStep 36200, Validation Loss: 3.3125\nStep 36400, Training loss: 3.256171703338623, Elapsed Time: 49.01 seconds\nStep 36400, Validation Loss: 3.3125\nStep 36600, Training loss: 3.261484384536743, Elapsed Time: 49.01 seconds\nStep 36600, Validation Loss: 3.234375\nStep 36800, Training loss: 3.255312442779541, Elapsed Time: 49.02 seconds\nStep 36800, Validation Loss: 3.265625\nStep 37000, Training loss: 3.253124952316284, Elapsed Time: 49.01 seconds\nStep 37000, Validation Loss: 3.203125\nStep 37200, Training loss: 3.251718759536743, Elapsed Time: 49.01 seconds\nStep 37200, Validation Loss: 3.25\nStep 37400, Training loss: 3.253046751022339, Elapsed Time: 49.01 seconds\nStep 37400, Validation Loss: 3.28125\nStep 37600, Training loss: 3.2459373474121094, Elapsed Time: 49.01 seconds\nStep 37600, Validation Loss: 3.28125\nStep 37800, Training loss: 3.255312442779541, Elapsed Time: 49.01 seconds\nStep 37800, Validation Loss: 3.234375\nStep 38000, Training loss: 3.2485156059265137, Elapsed Time: 49.00 seconds\nStep 38000, Validation Loss: 3.25\nStep 38200, Training loss: 3.2529687881469727, Elapsed Time: 49.01 seconds\nStep 38200, Validation Loss: 3.21875\nStep 38400, Training loss: 3.25390625, Elapsed Time: 49.00 seconds\nStep 38400, Validation Loss: 3.203125\nStep 38600, Training loss: 3.250312328338623, Elapsed Time: 49.00 seconds\nStep 38600, Validation Loss: 3.28125\nStep 38800, Training loss: 3.246328115463257, Elapsed Time: 49.01 seconds\nStep 38800, Validation Loss: 3.28125\nStep 39000, Training loss: 3.2406249046325684, Elapsed Time: 49.01 seconds\nStep 39000, Validation Loss: 3.265625\nStep 39200, Training loss: 3.241328001022339, Elapsed Time: 49.00 seconds\nStep 39200, Validation Loss: 3.328125\nStep 39400, Training loss: 3.2431249618530273, Elapsed Time: 49.01 seconds\nStep 39400, Validation Loss: 3.203125\nStep 39600, Training loss: 3.234062433242798, Elapsed Time: 49.01 seconds\nStep 39600, Validation Loss: 3.25\nStep 39800, Training loss: 3.2390623092651367, Elapsed Time: 49.02 seconds\nStep 39800, Validation Loss: 3.265625\nStep 40000, Training loss: 3.24078106880188, Elapsed Time: 49.00 seconds\nStep 40000, Validation Loss: 3.28125\nStep 40200, Training loss: 3.239374876022339, Elapsed Time: 49.02 seconds\nStep 40200, Validation Loss: 3.171875\nStep 40400, Training loss: 3.240859270095825, Elapsed Time: 49.01 seconds\nStep 40400, Validation Loss: 3.28125\nStep 40600, Training loss: 3.234452962875366, Elapsed Time: 49.00 seconds\nStep 40600, Validation Loss: 3.265625\nStep 40800, Training loss: 3.23296856880188, Elapsed Time: 49.00 seconds\nStep 40800, Validation Loss: 3.28125\nStep 41000, Training loss: 3.2312498092651367, Elapsed Time: 49.01 seconds\nStep 41000, Validation Loss: 3.28125\nStep 41200, Training loss: 3.2253124713897705, Elapsed Time: 49.00 seconds\nStep 41200, Validation Loss: 3.296875\nStep 41400, Training loss: 3.236328125, Elapsed Time: 49.00 seconds\nStep 41400, Validation Loss: 3.296875\nStep 41600, Training loss: 3.2339062690734863, Elapsed Time: 49.02 seconds\nStep 41600, Validation Loss: 3.171875\nStep 41800, Training loss: 3.226015567779541, Elapsed Time: 49.00 seconds\nStep 41800, Validation Loss: 3.21875\nStep 42000, Training loss: 3.2289843559265137, Elapsed Time: 49.01 seconds\nStep 42000, Validation Loss: 3.28125\nStep 42200, Training loss: 3.23296856880188, Elapsed Time: 49.00 seconds\nStep 42200, Validation Loss: 3.1875\nStep 42400, Training loss: 3.2300000190734863, Elapsed Time: 49.01 seconds\nStep 42400, Validation Loss: 3.265625\nStep 42600, Training loss: 3.2221875190734863, Elapsed Time: 49.00 seconds\nStep 42600, Validation Loss: 3.21875\nStep 42800, Training loss: 3.219374895095825, Elapsed Time: 49.03 seconds\nStep 42800, Validation Loss: 3.234375\nStep 43000, Training loss: 3.2208592891693115, Elapsed Time: 49.09 seconds\nStep 43000, Validation Loss: 3.25\nStep 43200, Training loss: 3.2208592891693115, Elapsed Time: 49.02 seconds\nStep 43200, Validation Loss: 3.1875\nStep 43400, Training loss: 3.219921827316284, Elapsed Time: 49.01 seconds\nStep 43400, Validation Loss: 3.171875\nStep 43600, Training loss: 3.217109203338623, Elapsed Time: 49.01 seconds\nStep 43600, Validation Loss: 3.203125\nStep 43800, Training loss: 3.2231249809265137, Elapsed Time: 49.02 seconds\nStep 43800, Validation Loss: 3.234375\nStep 44000, Training loss: 3.2130467891693115, Elapsed Time: 49.01 seconds\nStep 44000, Validation Loss: 3.15625\nStep 44200, Training loss: 3.2099218368530273, Elapsed Time: 49.00 seconds\nStep 44200, Validation Loss: 3.21875\nStep 44400, Training loss: 3.212890625, Elapsed Time: 49.00 seconds\nStep 44400, Validation Loss: 3.234375\nStep 44600, Training loss: 3.21539044380188, Elapsed Time: 49.01 seconds\nStep 44600, Validation Loss: 3.1875\nStep 44800, Training loss: 3.2121875286102295, Elapsed Time: 49.01 seconds\nStep 44800, Validation Loss: 3.203125\nStep 45000, Training loss: 3.2066404819488525, Elapsed Time: 49.01 seconds\nStep 45000, Validation Loss: 3.171875\nStep 45200, Training loss: 3.2117185592651367, Elapsed Time: 49.02 seconds\nStep 45200, Validation Loss: 3.234375\nStep 45400, Training loss: 3.207656145095825, Elapsed Time: 49.01 seconds\nStep 45400, Validation Loss: 3.1875\nStep 45600, Training loss: 3.2097654342651367, Elapsed Time: 49.00 seconds\nStep 45600, Validation Loss: 3.25\nStep 45800, Training loss: 3.2085936069488525, Elapsed Time: 49.01 seconds\nStep 45800, Validation Loss: 3.265625\nStep 46000, Training loss: 3.2004687786102295, Elapsed Time: 49.00 seconds\nStep 46000, Validation Loss: 3.21875\nStep 46200, Training loss: 3.2091405391693115, Elapsed Time: 49.01 seconds\nStep 46200, Validation Loss: 3.234375\nStep 46400, Training loss: 3.1996092796325684, Elapsed Time: 49.01 seconds\nStep 46400, Validation Loss: 3.171875\nStep 46600, Training loss: 3.202812433242798, Elapsed Time: 49.02 seconds\nStep 46600, Validation Loss: 3.21875\nStep 46800, Training loss: 3.2043750286102295, Elapsed Time: 49.00 seconds\nStep 46800, Validation Loss: 3.265625\nStep 47000, Training loss: 3.1996874809265137, Elapsed Time: 49.01 seconds\nStep 47000, Validation Loss: 3.171875\nStep 47200, Training loss: 3.1985936164855957, Elapsed Time: 49.01 seconds\nStep 47200, Validation Loss: 3.234375\nStep 47400, Training loss: 3.203749895095825, Elapsed Time: 49.00 seconds\nStep 47400, Validation Loss: 3.109375\nStep 47600, Training loss: 3.194999933242798, Elapsed Time: 49.02 seconds\nStep 47600, Validation Loss: 3.234375\nStep 47800, Training loss: 3.1962499618530273, Elapsed Time: 49.01 seconds\nStep 47800, Validation Loss: 3.1875\nStep 48000, Training loss: 3.19195294380188, Elapsed Time: 49.01 seconds\nStep 48000, Validation Loss: 3.171875\nStep 48200, Training loss: 3.19585919380188, Elapsed Time: 49.01 seconds\nStep 48200, Validation Loss: 3.265625\nStep 48400, Training loss: 3.1876561641693115, Elapsed Time: 49.00 seconds\nStep 48400, Validation Loss: 3.203125\nStep 48600, Training loss: 3.1971874237060547, Elapsed Time: 49.00 seconds\nStep 48600, Validation Loss: 3.28125\nStep 48800, Training loss: 3.1915624141693115, Elapsed Time: 49.00 seconds\nStep 48800, Validation Loss: 3.109375\nStep 49000, Training loss: 3.1901562213897705, Elapsed Time: 49.00 seconds\nStep 49000, Validation Loss: 3.28125\nStep 49200, Training loss: 3.1932029724121094, Elapsed Time: 49.02 seconds\nStep 49200, Validation Loss: 3.28125\nStep 49400, Training loss: 3.1874217987060547, Elapsed Time: 49.01 seconds\nStep 49400, Validation Loss: 3.203125\nStep 49600, Training loss: 3.1850781440734863, Elapsed Time: 49.01 seconds\nStep 49600, Validation Loss: 3.1875\nStep 49800, Training loss: 3.186718702316284, Elapsed Time: 49.01 seconds\nStep 49800, Validation Loss: 3.21875\nStep 50000, Training loss: 3.1828906536102295, Elapsed Time: 49.01 seconds\nStep 50000, Validation Loss: 3.171875\nStep 50200, Training loss: 3.1826562881469727, Elapsed Time: 49.01 seconds\nStep 50200, Validation Loss: 3.09375\nStep 50400, Training loss: 3.181952953338623, Elapsed Time: 49.01 seconds\nStep 50400, Validation Loss: 3.15625\nStep 50600, Training loss: 3.185546875, Elapsed Time: 49.01 seconds\nStep 50600, Validation Loss: 3.15625\nStep 50800, Training loss: 3.183046817779541, Elapsed Time: 49.01 seconds\nStep 50800, Validation Loss: 3.21875\nStep 51000, Training loss: 3.180468797683716, Elapsed Time: 49.01 seconds\nStep 51000, Validation Loss: 3.21875\nStep 51200, Training loss: 3.1809375286102295, Elapsed Time: 49.01 seconds\nStep 51200, Validation Loss: 3.171875\nStep 51400, Training loss: 3.1784374713897705, Elapsed Time: 49.00 seconds\nStep 51400, Validation Loss: 3.171875\nStep 51600, Training loss: 3.1725780963897705, Elapsed Time: 49.01 seconds\nStep 51600, Validation Loss: 3.140625\nStep 51800, Training loss: 3.175234317779541, Elapsed Time: 49.01 seconds\nStep 51800, Validation Loss: 3.171875\nStep 52000, Training loss: 3.1790623664855957, Elapsed Time: 49.01 seconds\nStep 52000, Validation Loss: 3.171875\nStep 52200, Training loss: 3.169687509536743, Elapsed Time: 49.00 seconds\nStep 52200, Validation Loss: 3.21875\nStep 52400, Training loss: 3.168828010559082, Elapsed Time: 49.00 seconds\nStep 52400, Validation Loss: 3.1875\nStep 52600, Training loss: 3.1725780963897705, Elapsed Time: 49.00 seconds\nStep 52600, Validation Loss: 3.171875\nStep 52800, Training loss: 3.171875, Elapsed Time: 49.01 seconds\nStep 52800, Validation Loss: 3.1875\nStep 53000, Training loss: 3.1686718463897705, Elapsed Time: 49.01 seconds\nStep 53000, Validation Loss: 3.25\nStep 53200, Training loss: 3.166093587875366, Elapsed Time: 49.01 seconds\nStep 53200, Validation Loss: 3.140625\nStep 53400, Training loss: 3.1728124618530273, Elapsed Time: 49.01 seconds\nStep 53400, Validation Loss: 3.234375\nStep 53600, Training loss: 3.1606249809265137, Elapsed Time: 49.01 seconds\nStep 53600, Validation Loss: 3.265625\nStep 53800, Training loss: 3.1622655391693115, Elapsed Time: 49.02 seconds\nStep 53800, Validation Loss: 3.1875\nStep 54000, Training loss: 3.1636717319488525, Elapsed Time: 49.00 seconds\nStep 54000, Validation Loss: 3.140625\nStep 54200, Training loss: 3.161328077316284, Elapsed Time: 49.01 seconds\nStep 54200, Validation Loss: 3.109375\nStep 54400, Training loss: 3.153203010559082, Elapsed Time: 49.00 seconds\nStep 54400, Validation Loss: 3.125\nStep 54600, Training loss: 3.156484365463257, Elapsed Time: 49.01 seconds\nStep 54600, Validation Loss: 3.234375\nStep 54800, Training loss: 3.15874981880188, Elapsed Time: 49.01 seconds\nStep 54800, Validation Loss: 3.078125\nStep 55000, Training loss: 3.156874895095825, Elapsed Time: 49.01 seconds\nStep 55000, Validation Loss: 3.171875\nStep 55200, Training loss: 3.158515453338623, Elapsed Time: 49.01 seconds\nStep 55200, Validation Loss: 3.140625\nStep 55400, Training loss: 3.1530468463897705, Elapsed Time: 49.01 seconds\nStep 55400, Validation Loss: 3.140625\nStep 55600, Training loss: 3.153125047683716, Elapsed Time: 49.01 seconds\nStep 55600, Validation Loss: 3.21875\nStep 55800, Training loss: 3.1649999618530273, Elapsed Time: 49.01 seconds\nStep 55800, Validation Loss: 3.125\nStep 56000, Training loss: 3.151249885559082, Elapsed Time: 49.01 seconds\nStep 56000, Validation Loss: 3.109375\nStep 56200, Training loss: 3.152109384536743, Elapsed Time: 49.01 seconds\nStep 56200, Validation Loss: 3.234375\nStep 56400, Training loss: 3.1536717414855957, Elapsed Time: 49.01 seconds\nStep 56400, Validation Loss: 3.15625\nStep 56600, Training loss: 3.1555469036102295, Elapsed Time: 49.01 seconds\nStep 56600, Validation Loss: 3.140625\nStep 56800, Training loss: 3.152968645095825, Elapsed Time: 49.01 seconds\nStep 56800, Validation Loss: 3.140625\nStep 57000, Training loss: 3.1471874713897705, Elapsed Time: 49.01 seconds\nStep 57000, Validation Loss: 3.25\nStep 57200, Training loss: 3.148124933242798, Elapsed Time: 49.02 seconds\nStep 57200, Validation Loss: 3.15625\nStep 57400, Training loss: 3.147109270095825, Elapsed Time: 49.01 seconds\nStep 57400, Validation Loss: 3.09375\nStep 57600, Training loss: 3.15234375, Elapsed Time: 49.01 seconds\nStep 57600, Validation Loss: 3.234375\nStep 57800, Training loss: 3.149609327316284, Elapsed Time: 49.01 seconds\nStep 57800, Validation Loss: 3.21875\nStep 58000, Training loss: 3.145156145095825, Elapsed Time: 49.02 seconds\nStep 58000, Validation Loss: 3.109375\nStep 58200, Training loss: 3.1435155868530273, Elapsed Time: 49.00 seconds\nStep 58200, Validation Loss: 3.140625\nStep 58400, Training loss: 3.140859365463257, Elapsed Time: 49.02 seconds\nStep 58400, Validation Loss: 3.078125\nStep 58600, Training loss: 3.140937328338623, Elapsed Time: 49.01 seconds\nStep 58600, Validation Loss: 3.21875\nStep 58800, Training loss: 3.1466405391693115, Elapsed Time: 49.01 seconds\nStep 58800, Validation Loss: 3.171875\nStep 59000, Training loss: 3.1432812213897705, Elapsed Time: 49.01 seconds\nStep 59000, Validation Loss: 3.140625\nStep 59200, Training loss: 3.131953001022339, Elapsed Time: 49.01 seconds\nStep 59200, Validation Loss: 3.140625\nStep 59400, Training loss: 3.144296884536743, Elapsed Time: 49.01 seconds\nStep 59400, Validation Loss: 3.078125\nStep 59600, Training loss: 3.132578134536743, Elapsed Time: 49.00 seconds\nStep 59600, Validation Loss: 3.125\nStep 59800, Training loss: 3.141796827316284, Elapsed Time: 49.01 seconds\nStep 59800, Validation Loss: 3.140625\nStep 60000, Training loss: 3.135859251022339, Elapsed Time: 49.01 seconds\nStep 60000, Validation Loss: 3.109375\nStep 60200, Training loss: 3.1332030296325684, Elapsed Time: 49.01 seconds\nStep 60200, Validation Loss: 3.15625\nStep 60400, Training loss: 3.135546922683716, Elapsed Time: 49.01 seconds\nStep 60400, Validation Loss: 3.1875\nStep 60600, Training loss: 3.1292967796325684, Elapsed Time: 49.00 seconds\nStep 60600, Validation Loss: 3.125\nStep 60800, Training loss: 3.129687547683716, Elapsed Time: 49.01 seconds\nStep 60800, Validation Loss: 3.125\nStep 61000, Training loss: 3.1288280487060547, Elapsed Time: 49.00 seconds\nStep 61000, Validation Loss: 3.109375\nStep 61200, Training loss: 3.1273436546325684, Elapsed Time: 49.00 seconds\nStep 61200, Validation Loss: 3.140625\nStep 61400, Training loss: 3.1298437118530273, Elapsed Time: 49.00 seconds\nStep 61400, Validation Loss: 3.1875\nStep 61600, Training loss: 3.1242969036102295, Elapsed Time: 49.00 seconds\nStep 61600, Validation Loss: 3.109375\nStep 61800, Training loss: 3.127187490463257, Elapsed Time: 49.02 seconds\nStep 61800, Validation Loss: 3.15625\nStep 62000, Training loss: 3.122812509536743, Elapsed Time: 49.00 seconds\nStep 62000, Validation Loss: 3.1875\nStep 62200, Training loss: 3.1228904724121094, Elapsed Time: 49.00 seconds\nStep 62200, Validation Loss: 3.140625\nStep 62400, Training loss: 3.127265453338623, Elapsed Time: 49.01 seconds\nStep 62400, Validation Loss: 3.046875\nStep 62600, Training loss: 3.1259374618530273, Elapsed Time: 49.01 seconds\nStep 62600, Validation Loss: 3.125\nStep 62800, Training loss: 3.1215624809265137, Elapsed Time: 49.00 seconds\nStep 62800, Validation Loss: 3.125\nStep 63000, Training loss: 3.112187385559082, Elapsed Time: 49.00 seconds\nStep 63000, Validation Loss: 3.0625\nStep 63200, Training loss: 3.121953010559082, Elapsed Time: 49.00 seconds\nStep 63200, Validation Loss: 3.109375\nStep 63400, Training loss: 3.1235156059265137, Elapsed Time: 49.00 seconds\nStep 63400, Validation Loss: 3.125\nStep 63600, Training loss: 3.1190624237060547, Elapsed Time: 49.00 seconds\nStep 63600, Validation Loss: 3.09375\nStep 63800, Training loss: 3.1117186546325684, Elapsed Time: 49.01 seconds\nStep 63800, Validation Loss: 3.125\nStep 64000, Training loss: 3.1164844036102295, Elapsed Time: 49.00 seconds\nStep 64000, Validation Loss: 3.1875\nStep 64200, Training loss: 3.1109373569488525, Elapsed Time: 49.01 seconds\nStep 64200, Validation Loss: 3.046875\nStep 64400, Training loss: 3.115312337875366, Elapsed Time: 49.00 seconds\nStep 64400, Validation Loss: 3.15625\nStep 64600, Training loss: 3.114062547683716, Elapsed Time: 49.01 seconds\nStep 64600, Validation Loss: 3.21875\nStep 64800, Training loss: 3.10992169380188, Elapsed Time: 49.02 seconds\nStep 64800, Validation Loss: 3.09375\nStep 65000, Training loss: 3.1092967987060547, Elapsed Time: 49.00 seconds\nStep 65000, Validation Loss: 3.046875\nStep 65200, Training loss: 3.112421751022339, Elapsed Time: 49.01 seconds\nStep 65200, Validation Loss: 3.09375\nStep 65400, Training loss: 3.1132030487060547, Elapsed Time: 49.02 seconds\nStep 65400, Validation Loss: 3.078125\nStep 65600, Training loss: 3.107421875, Elapsed Time: 49.01 seconds\nStep 65600, Validation Loss: 3.203125\nStep 65800, Training loss: 3.107499837875366, Elapsed Time: 49.00 seconds\nStep 65800, Validation Loss: 3.140625\nStep 66000, Training loss: 3.1084375381469727, Elapsed Time: 49.01 seconds\nStep 66000, Validation Loss: 3.03125\nStep 66200, Training loss: 3.106562376022339, Elapsed Time: 49.01 seconds\nStep 66200, Validation Loss: 3.0625\nStep 66400, Training loss: 3.10796856880188, Elapsed Time: 49.01 seconds\nStep 66400, Validation Loss: 3.140625\nStep 66600, Training loss: 3.1059374809265137, Elapsed Time: 49.01 seconds\nStep 66600, Validation Loss: 3.015625\nStep 66800, Training loss: 3.1089842319488525, Elapsed Time: 49.01 seconds\nStep 66800, Validation Loss: 3.109375\nStep 67000, Training loss: 3.106250047683716, Elapsed Time: 49.01 seconds\nStep 67000, Validation Loss: 3.125\nStep 67200, Training loss: 3.105234384536743, Elapsed Time: 49.00 seconds\nStep 67200, Validation Loss: 3.171875\nStep 67400, Training loss: 3.098828077316284, Elapsed Time: 49.00 seconds\nStep 67400, Validation Loss: 3.09375\nStep 67600, Training loss: 3.1025781631469727, Elapsed Time: 49.00 seconds\nStep 67600, Validation Loss: 3.125\nStep 67800, Training loss: 3.097109317779541, Elapsed Time: 49.01 seconds\nStep 67800, Validation Loss: 3.09375\nStep 68000, Training loss: 3.097968578338623, Elapsed Time: 49.01 seconds\nStep 68000, Validation Loss: 3.109375\nStep 68200, Training loss: 3.0980467796325684, Elapsed Time: 49.01 seconds\nStep 68200, Validation Loss: 3.109375\nStep 68400, Training loss: 3.0969531536102295, Elapsed Time: 49.01 seconds\nStep 68400, Validation Loss: 3.078125\nStep 68600, Training loss: 3.094921827316284, Elapsed Time: 49.02 seconds\nStep 68600, Validation Loss: 3.15625\nStep 68800, Training loss: 3.092109203338623, Elapsed Time: 49.01 seconds\nStep 68800, Validation Loss: 3.109375\nStep 69000, Training loss: 3.0928125381469727, Elapsed Time: 49.02 seconds\nStep 69000, Validation Loss: 3.046875\nStep 69200, Training loss: 3.0908594131469727, Elapsed Time: 49.01 seconds\nStep 69200, Validation Loss: 3.15625\nStep 69400, Training loss: 3.0935935974121094, Elapsed Time: 49.01 seconds\nStep 69400, Validation Loss: 3.03125\nStep 69600, Training loss: 3.09375, Elapsed Time: 49.02 seconds\nStep 69600, Validation Loss: 3.09375\nStep 69800, Training loss: 3.0891406536102295, Elapsed Time: 49.01 seconds\nStep 69800, Validation Loss: 3.109375\nStep 70000, Training loss: 3.094531297683716, Elapsed Time: 49.01 seconds\nStep 70000, Validation Loss: 3.078125\nStep 70200, Training loss: 3.085390567779541, Elapsed Time: 49.01 seconds\nStep 70200, Validation Loss: 3.109375\nStep 70400, Training loss: 3.087656259536743, Elapsed Time: 49.01 seconds\nStep 70400, Validation Loss: 3.0625\nStep 70600, Training loss: 3.0874998569488525, Elapsed Time: 49.03 seconds\nStep 70600, Validation Loss: 3.046875\nStep 70800, Training loss: 3.089531183242798, Elapsed Time: 49.02 seconds\nStep 70800, Validation Loss: 3.078125\nStep 71000, Training loss: 3.0883593559265137, Elapsed Time: 49.02 seconds\nStep 71000, Validation Loss: 3.21875\nStep 71200, Training loss: 3.0891406536102295, Elapsed Time: 49.01 seconds\nStep 71200, Validation Loss: 3.0625\nStep 71400, Training loss: 3.0855467319488525, Elapsed Time: 49.01 seconds\nStep 71400, Validation Loss: 3.15625\nStep 71600, Training loss: 3.0844531059265137, Elapsed Time: 49.02 seconds\nStep 71600, Validation Loss: 3.078125\nStep 71800, Training loss: 3.0782811641693115, Elapsed Time: 49.01 seconds\nStep 71800, Validation Loss: 3.109375\nStep 72000, Training loss: 3.081796884536743, Elapsed Time: 49.01 seconds\nStep 72000, Validation Loss: 3.0\nStep 72200, Training loss: 3.082109212875366, Elapsed Time: 49.01 seconds\nStep 72200, Validation Loss: 3.09375\nStep 72400, Training loss: 3.0835156440734863, Elapsed Time: 49.01 seconds\nStep 72400, Validation Loss: 3.109375\nStep 72600, Training loss: 3.0810155868530273, Elapsed Time: 49.01 seconds\nStep 72600, Validation Loss: 3.078125\nStep 72800, Training loss: 3.080859422683716, Elapsed Time: 49.01 seconds\nStep 72800, Validation Loss: 3.125\nStep 73000, Training loss: 3.0833592414855957, Elapsed Time: 49.01 seconds\nStep 73000, Validation Loss: 3.125\nStep 73200, Training loss: 3.0757811069488525, Elapsed Time: 49.01 seconds\nStep 73200, Validation Loss: 3.109375\nStep 73400, Training loss: 3.075859308242798, Elapsed Time: 49.01 seconds\nStep 73400, Validation Loss: 3.15625\nStep 73600, Training loss: 3.0824999809265137, Elapsed Time: 49.01 seconds\nStep 73600, Validation Loss: 3.125\nStep 73800, Training loss: 3.083671808242798, Elapsed Time: 49.01 seconds\nStep 73800, Validation Loss: 3.046875\nStep 74000, Training loss: 3.0727343559265137, Elapsed Time: 49.01 seconds\nStep 74000, Validation Loss: 3.0625\nStep 74200, Training loss: 3.075312376022339, Elapsed Time: 49.01 seconds\nStep 74200, Validation Loss: 3.0625\nStep 74400, Training loss: 3.071484327316284, Elapsed Time: 49.01 seconds\nStep 74400, Validation Loss: 3.09375\nStep 74600, Training loss: 3.072265625, Elapsed Time: 49.01 seconds\nStep 74600, Validation Loss: 3.078125\nStep 74800, Training loss: 3.07421875, Elapsed Time: 49.00 seconds\nStep 74800, Validation Loss: 3.109375\nStep 75000, Training loss: 3.0703125, Elapsed Time: 49.00 seconds\nStep 75000, Validation Loss: 3.09375\nStep 75200, Training loss: 3.0693750381469727, Elapsed Time: 49.00 seconds\nStep 75200, Validation Loss: 3.015625\nStep 75400, Training loss: 3.06640625, Elapsed Time: 49.02 seconds\nStep 75400, Validation Loss: 3.15625\nStep 75600, Training loss: 3.0685155391693115, Elapsed Time: 49.01 seconds\nStep 75600, Validation Loss: 3.046875\nStep 75800, Training loss: 3.070078134536743, Elapsed Time: 49.01 seconds\nStep 75800, Validation Loss: 3.0625\nStep 76000, Training loss: 3.0718748569488525, Elapsed Time: 49.01 seconds\nStep 76000, Validation Loss: 3.09375\nStep 76200, Training loss: 3.071718692779541, Elapsed Time: 49.01 seconds\nStep 76200, Validation Loss: 3.15625\nStep 76400, Training loss: 3.065624952316284, Elapsed Time: 49.00 seconds\nStep 76400, Validation Loss: 3.046875\nStep 76600, Training loss: 3.0665624141693115, Elapsed Time: 49.01 seconds\nStep 76600, Validation Loss: 3.109375\nStep 76800, Training loss: 3.0646092891693115, Elapsed Time: 49.02 seconds\nStep 76800, Validation Loss: 3.03125\nStep 77000, Training loss: 3.069765567779541, Elapsed Time: 49.02 seconds\nStep 77000, Validation Loss: 3.09375\nStep 77200, Training loss: 3.0703125, Elapsed Time: 49.01 seconds\nStep 77200, Validation Loss: 3.078125\nStep 77400, Training loss: 3.065078020095825, Elapsed Time: 49.02 seconds\nStep 77400, Validation Loss: 3.09375\nStep 77600, Training loss: 3.066640615463257, Elapsed Time: 49.00 seconds\nStep 77600, Validation Loss: 3.140625\nStep 77800, Training loss: 3.064218759536743, Elapsed Time: 49.00 seconds\nStep 77800, Validation Loss: 3.109375\nStep 78000, Training loss: 3.0649218559265137, Elapsed Time: 49.01 seconds\nStep 78000, Validation Loss: 3.046875\nStep 78200, Training loss: 3.066093683242798, Elapsed Time: 49.01 seconds\nStep 78200, Validation Loss: 3.09375\nStep 78400, Training loss: 3.065234422683716, Elapsed Time: 49.01 seconds\nStep 78400, Validation Loss: 3.0\nStep 78600, Training loss: 3.063593626022339, Elapsed Time: 49.00 seconds\nStep 78600, Validation Loss: 3.140625\nStep 78800, Training loss: 3.0575780868530273, Elapsed Time: 49.01 seconds\nStep 78800, Validation Loss: 3.109375\nStep 79000, Training loss: 3.059218645095825, Elapsed Time: 49.02 seconds\nStep 79000, Validation Loss: 3.03125\nStep 79200, Training loss: 3.0618748664855957, Elapsed Time: 49.00 seconds\nStep 79200, Validation Loss: 3.0625\nStep 79400, Training loss: 3.061640501022339, Elapsed Time: 49.00 seconds\nStep 79400, Validation Loss: 3.046875\nStep 79600, Training loss: 3.0637500286102295, Elapsed Time: 49.02 seconds\nStep 79600, Validation Loss: 3.03125\nStep 79800, Training loss: 3.0665624141693115, Elapsed Time: 49.00 seconds\nStep 79800, Validation Loss: 3.046875\nStep 80000, Training loss: 3.058671712875366, Elapsed Time: 49.01 seconds\nStep 80000, Validation Loss: 3.046875\nStep 80200, Training loss: 3.058046817779541, Elapsed Time: 49.02 seconds\nStep 80200, Validation Loss: 3.0\nStep 80400, Training loss: 3.05914044380188, Elapsed Time: 49.01 seconds\nStep 80400, Validation Loss: 3.03125\nStep 80600, Training loss: 3.059375047683716, Elapsed Time: 49.01 seconds\nStep 80600, Validation Loss: 3.09375\nStep 80800, Training loss: 3.0610156059265137, Elapsed Time: 49.00 seconds\nStep 80800, Validation Loss: 3.125\nStep 81000, Training loss: 3.0623435974121094, Elapsed Time: 49.00 seconds\nStep 81000, Validation Loss: 3.0\nStep 81200, Training loss: 3.05718731880188, Elapsed Time: 49.01 seconds\nStep 81200, Validation Loss: 3.046875\nStep 81400, Training loss: 3.0546875, Elapsed Time: 49.01 seconds\nStep 81400, Validation Loss: 3.09375\nStep 81600, Training loss: 3.0537500381469727, Elapsed Time: 49.00 seconds\nStep 81600, Validation Loss: 3.15625\nStep 81800, Training loss: 3.0570311546325684, Elapsed Time: 49.01 seconds\nStep 81800, Validation Loss: 3.03125\nStep 82000, Training loss: 3.053046703338623, Elapsed Time: 49.00 seconds\nStep 82000, Validation Loss: 3.078125\nStep 82200, Training loss: 3.056093692779541, Elapsed Time: 49.00 seconds\nStep 82200, Validation Loss: 3.0625\nStep 82400, Training loss: 3.0628905296325684, Elapsed Time: 49.01 seconds\nStep 82400, Validation Loss: 3.109375\nStep 82600, Training loss: 3.063671827316284, Elapsed Time: 49.00 seconds\nStep 82600, Validation Loss: 3.046875\nStep 82800, Training loss: 3.0585155487060547, Elapsed Time: 49.00 seconds\nStep 82800, Validation Loss: 3.015625\nStep 83000, Training loss: 3.0562498569488525, Elapsed Time: 49.01 seconds\nStep 83000, Validation Loss: 3.109375\nStep 83200, Training loss: 3.0578906536102295, Elapsed Time: 49.00 seconds\nStep 83200, Validation Loss: 3.09375\nStep 83400, Training loss: 3.051953077316284, Elapsed Time: 49.00 seconds\nStep 83400, Validation Loss: 3.171875\nStep 83600, Training loss: 3.063906192779541, Elapsed Time: 49.01 seconds\nStep 83600, Validation Loss: 3.0625\nStep 83800, Training loss: 3.058906078338623, Elapsed Time: 49.01 seconds\nStep 83800, Validation Loss: 3.03125\nStep 84000, Training loss: 3.0517969131469727, Elapsed Time: 49.01 seconds\nStep 84000, Validation Loss: 3.078125\nStep 84200, Training loss: 3.0546092987060547, Elapsed Time: 49.01 seconds\nStep 84200, Validation Loss: 3.03125\nStep 84400, Training loss: 3.0514843463897705, Elapsed Time: 49.00 seconds\nStep 84400, Validation Loss: 3.015625\nStep 84600, Training loss: 3.0511717796325684, Elapsed Time: 49.00 seconds\nStep 84600, Validation Loss: 3.09375\nStep 84800, Training loss: 3.054921865463257, Elapsed Time: 49.00 seconds\nStep 84800, Validation Loss: 3.03125\nStep 85000, Training loss: 3.0528905391693115, Elapsed Time: 49.00 seconds\nStep 85000, Validation Loss: 3.09375\nStep 85200, Training loss: 3.058281183242798, Elapsed Time: 49.00 seconds\nStep 85200, Validation Loss: 3.09375\nStep 85400, Training loss: 3.0532031059265137, Elapsed Time: 49.02 seconds\nStep 85400, Validation Loss: 3.03125\nStep 85600, Training loss: 3.0540623664855957, Elapsed Time: 49.00 seconds\nStep 85600, Validation Loss: 3.03125\nStep 85800, Training loss: 3.053828001022339, Elapsed Time: 49.01 seconds\nStep 85800, Validation Loss: 3.140625\nStep 86000, Training loss: 3.05523419380188, Elapsed Time: 49.01 seconds\nStep 86000, Validation Loss: 3.078125\nStep 86200, Training loss: 3.057499885559082, Elapsed Time: 49.00 seconds\nStep 86200, Validation Loss: 3.03125\nStep 86400, Training loss: 3.0492186546325684, Elapsed Time: 49.01 seconds\nStep 86400, Validation Loss: 3.0\nStep 86600, Training loss: 3.051953077316284, Elapsed Time: 49.00 seconds\nStep 86600, Validation Loss: 3.125\nStep 86800, Training loss: 3.053359270095825, Elapsed Time: 49.00 seconds\nStep 86800, Validation Loss: 3.15625\nStep 87000, Training loss: 3.0478906631469727, Elapsed Time: 49.00 seconds\nStep 87000, Validation Loss: 3.109375\nStep 87200, Training loss: 3.053359270095825, Elapsed Time: 49.00 seconds\nStep 87200, Validation Loss: 3.015625\nStep 87400, Training loss: 3.053046703338623, Elapsed Time: 49.01 seconds\nStep 87400, Validation Loss: 3.0625\nStep 87600, Training loss: 3.049921751022339, Elapsed Time: 49.01 seconds\nStep 87600, Validation Loss: 3.09375\nStep 87800, Training loss: 3.0525779724121094, Elapsed Time: 49.01 seconds\nStep 87800, Validation Loss: 3.125\nStep 88000, Training loss: 3.05859375, Elapsed Time: 49.00 seconds\nStep 88000, Validation Loss: 3.03125\nStep 88200, Training loss: 3.056093692779541, Elapsed Time: 49.01 seconds\nStep 88200, Validation Loss: 3.03125\nStep 88400, Training loss: 3.055781126022339, Elapsed Time: 49.01 seconds\nStep 88400, Validation Loss: 3.140625\nStep 88600, Training loss: 3.0556249618530273, Elapsed Time: 49.01 seconds\nStep 88600, Validation Loss: 3.125\nStep 88800, Training loss: 3.052500009536743, Elapsed Time: 49.01 seconds\nStep 88800, Validation Loss: 3.0\nStep 89000, Training loss: 3.04937481880188, Elapsed Time: 49.01 seconds\nStep 89000, Validation Loss: 3.09375\nStep 89200, Training loss: 3.049687385559082, Elapsed Time: 49.00 seconds\nStep 89200, Validation Loss: 3.078125\nStep 89400, Training loss: 3.052734375, Elapsed Time: 49.00 seconds\nStep 89400, Validation Loss: 3.140625\nStep 89600, Training loss: 3.055781126022339, Elapsed Time: 49.01 seconds\nStep 89600, Validation Loss: 3.078125\nStep 89800, Training loss: 3.052187442779541, Elapsed Time: 49.01 seconds\nStep 89800, Validation Loss: 3.046875\nStep 90000, Training loss: 3.0545310974121094, Elapsed Time: 49.01 seconds\nStep 90000, Validation Loss: 3.078125\nFinal generated text:\nOnce upon a time",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the training loss."
      ],
      "metadata": {
        "id": "thaLs6TD0lt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6Eg1Cz2y_iP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:43:50.029672Z",
          "iopub.execute_input": "2025-02-21T10:43:50.030074Z",
          "iopub.status.idle": "2025-02-21T10:43:51.584464Z",
          "shell.execute_reply.started": "2025-02-21T10:43:50.030047Z",
          "shell.execute_reply": "2025-02-21T10:43:51.583724Z"
        },
        "outputId": "2ff3ead4-43a6-4d76-bd61-72ed8a7c9d3d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible sentences at the end of the training.\n",
        "\n"
      ],
      "metadata": {
        "id": "26_HtTkZTczd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some training results ([wandb log](https://www.google.com/url?q=https%3A%2F%2Fwandb.ai%2Fwindmaple%2FGPT2-pretraining%3Fnw%3Dnwuserwindmaple)):\n",
        "\n",
        "| model | params | train loss | val loss | training time (TPU v3)\n",
        "| ------| ------ | ---------- | -------- | -----------------------\n",
        "| gpt2 | 124M         | 3.05  | 3.09     | 6.5 hr\n",
        "| gpt2-medium | 354M  | 2.83  | 2.86     | 22.5 hr\n",
        "\n",
        "The losses are roughly in line with [nanoGPT's](https://github.com/karpathy/nanoGPT?tab=readme-ov-file#baselines)."
      ],
      "metadata": {
        "id": "9hTE4MsEUh9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model saving"
      ],
      "metadata": {
        "id": "m13hUf7CcrSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import orbax.checkpoint as orbax\n",
        "import shutil\n",
        "\n",
        "if platform == \"Colab\":\n",
        "  checkpoint_path = \"/content/checkpoints\"\n",
        "elif platform == \"Kaggle\":\n",
        "  checkpoint_path = \"/kaggle/working/checkpoints\"\n",
        "else:\n",
        "  from pathlib import Path\n",
        "  home = Path.home()\n",
        "  checkpoint_path = os.path.join(str(home), \"checkpoints\")\n",
        "\n",
        "# make sure the folder is empty and usable\n",
        "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "train_state = nnx.state(model)\n",
        "checkpointer.save(checkpoint_path, train_state)"
      ],
      "metadata": {
        "id": "ofH_VkqMctZ5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:43:51.585651Z",
          "iopub.execute_input": "2025-02-21T10:43:51.586015Z",
          "iopub.status.idle": "2025-02-21T10:43:57.582123Z",
          "shell.execute_reply.started": "2025-02-21T10:43:51.585991Z",
          "shell.execute_reply": "2025-02-21T10:43:57.581319Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model restoration\n",
        "Restore the model checkpoint. A pretrained checkpoint is also available [here](https://www.kaggle.com/models/windmaple/gpt2/)."
      ],
      "metadata": {
        "id": "soPqiR1JNmjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nnx.eval_shape(lambda: create_model(rngs=nnx.Rngs(0)))\n",
        "state = nnx.state(model)\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "state = checkpointer.restore(checkpoint_path, item=state)\n",
        "nnx.update(model, state)\n",
        "\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Restored model generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "EkoFGCgSZ1yz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:43:57.583415Z",
          "iopub.execute_input": "2025-02-21T10:43:57.583673Z",
          "iopub.status.idle": "2025-02-21T10:44:12.704383Z",
          "shell.execute_reply.started": "2025-02-21T10:43:57.583648Z",
          "shell.execute_reply": "2025-02-21T10:44:12.703322Z"
        },
        "outputId": "cb39c0b9-05f2-44f9-fc50-b94571ea2765"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/site-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1167: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Restored model generated text:\nOnce upon a time, I was a very young girl. I was very young, but I was a very young girl. I was a very young girl. I was a very young girl. I was a very young girl. I was a very young girl. I was a very young girl. I was a very young girl. I was a very young girl. I was a very young girl. I was a very young girl. I was a very young girl. I was a very young girl. I was a very\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disconnect the Colab runtime"
      ],
      "metadata": {
        "id": "jCApVd7671c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import runtime\n",
        "  runtime.unassign()"
      ],
      "metadata": {
        "id": "NsqYdbrDVKSq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:44:12.705516Z",
          "iopub.execute_input": "2025-02-21T10:44:12.705749Z",
          "iopub.status.idle": "2025-02-21T10:44:12.710493Z",
          "shell.execute_reply.started": "2025-02-21T10:44:12.705726Z",
          "shell.execute_reply": "2025-02-21T10:44:12.709284Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}